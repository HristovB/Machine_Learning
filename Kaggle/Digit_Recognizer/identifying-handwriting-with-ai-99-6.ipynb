{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022884,
     "end_time": "2021-01-14T21:32:52.569438",
     "exception": false,
     "start_time": "2021-01-14T21:32:52.546554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# AI leads the way to computer vision\n",
    "\n",
    "Most traditional methods of analysing and processing of images and video have been quickly surpassed by the power of artificial intelligence and deep learning. Convolutional Neural Networks, that work on the same basic principal as the neurons in the visual cortex, can analyse images and identify objects in them with staggering accuracy.\n",
    "\n",
    "This is why for the solution to the problem of recognizing handwritten digits, CNNs are the best approach to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021308,
     "end_time": "2021-01-14T21:32:52.612187",
     "exception": false,
     "start_time": "2021-01-14T21:32:52.590879",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Importing the libraries\n",
    "\n",
    "First we need to import the basic libraries that we will use for preprocessing and visualization of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T21:32:52.662302Z",
     "iopub.status.busy": "2021-01-14T21:32:52.661480Z",
     "iopub.status.idle": "2021-01-14T21:32:52.664417Z",
     "shell.execute_reply": "2021-01-14T21:32:52.663789Z"
    },
    "papermill": {
     "duration": 0.030785,
     "end_time": "2021-01-14T21:32:52.664515",
     "exception": false,
     "start_time": "2021-01-14T21:32:52.633730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020814,
     "end_time": "2021-01-14T21:32:52.707135",
     "exception": false,
     "start_time": "2021-01-14T21:32:52.686321",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "Next, we load the training and test datasets into separate variables using *Pandas*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-01-14T21:32:52.752465Z",
     "iopub.status.busy": "2021-01-14T21:32:52.751484Z",
     "iopub.status.idle": "2021-01-14T21:32:59.854582Z",
     "shell.execute_reply": "2021-01-14T21:32:59.855115Z"
    },
    "papermill": {
     "duration": 7.127009,
     "end_time": "2021-01-14T21:32:59.855281",
     "exception": false,
     "start_time": "2021-01-14T21:32:52.728272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('../input/digit-recognizer/train.csv')\n",
    "test_data = pd.read_csv('../input/digit-recognizer/test.csv')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024536,
     "end_time": "2021-01-14T21:32:59.905072",
     "exception": false,
     "start_time": "2021-01-14T21:32:59.880536",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can see that the training dataset consists of one column for the label (which represents what digit was written), and 784 columns for each pixel of the image that contains the handwritten digit. To be able to use a 2D Convolutional Neural Network, we need to reshape the data so we can get the actual image of the digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T21:32:59.960263Z",
     "iopub.status.busy": "2021-01-14T21:32:59.959482Z",
     "iopub.status.idle": "2021-01-14T21:32:59.965861Z",
     "shell.execute_reply": "2021-01-14T21:32:59.965194Z"
    },
    "papermill": {
     "duration": 0.035845,
     "end_time": "2021-01-14T21:32:59.965994",
     "exception": false,
     "start_time": "2021-01-14T21:32:59.930149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024887,
     "end_time": "2021-01-14T21:33:00.015965",
     "exception": false,
     "start_time": "2021-01-14T21:32:59.991078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reshaping the data\n",
    "\n",
    "The current shape of the training dataset given above tells us that there are 42000 images which are represented by 784 pixels (plus one column for the label). That means that in order to get a perfect square image, the dimensions of the pixel need to be ($\\sqrt{784}, \\sqrt{784}$), or (28, 28) pixels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023751,
     "end_time": "2021-01-14T21:33:00.064191",
     "exception": false,
     "start_time": "2021-01-14T21:33:00.040440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Separating the label column\n",
    "\n",
    "In order to reshape the data, first we must separate the labels from the feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T21:33:00.203516Z",
     "iopub.status.busy": "2021-01-14T21:33:00.201314Z",
     "iopub.status.idle": "2021-01-14T21:33:00.204388Z",
     "shell.execute_reply": "2021-01-14T21:33:00.204930Z"
    },
    "papermill": {
     "duration": 0.116599,
     "end_time": "2021-01-14T21:33:00.205076",
     "exception": false,
     "start_time": "2021-01-14T21:33:00.088477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_labels = train_data['label']\n",
    "train_data = train_data.drop('label', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026937,
     "end_time": "2021-01-14T21:33:00.256770",
     "exception": false,
     "start_time": "2021-01-14T21:33:00.229833",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Reshaping the datasets\n",
    "\n",
    "As mentioned previously, the required shape of each image is 28x28 pixels, and we can easily achieve that using the *NumPy* function *reshape*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-01-14T21:33:00.316185Z",
     "iopub.status.busy": "2021-01-14T21:33:00.315111Z",
     "iopub.status.idle": "2021-01-14T21:33:00.320739Z",
     "shell.execute_reply": "2021-01-14T21:33:00.320160Z"
    },
    "papermill": {
     "duration": 0.037622,
     "end_time": "2021-01-14T21:33:00.320843",
     "exception": false,
     "start_time": "2021-01-14T21:33:00.283221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.values[:, :, np.newaxis].reshape(-1, 28, 28)\n",
    "test_data = test_data.values[:, :, np.newaxis].reshape(-1, 28, 28)\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024999,
     "end_time": "2021-01-14T21:33:00.371219",
     "exception": false,
     "start_time": "2021-01-14T21:33:00.346220",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can observe that the training data now has a shape of (42000, 28, 28), which represents the 42000 images of handwritten digits, with dimensions 28x28."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026039,
     "end_time": "2021-01-14T21:33:00.422657",
     "exception": false,
     "start_time": "2021-01-14T21:33:00.396618",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Visualization of the data\n",
    "\n",
    "Before moving on, we can visualize the images of the digits to get a better understanding of how they look. This can be easily done using the *matplotlib* library.\n",
    "\n",
    "We will plot 9 random images from the dataset that will be picked at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T21:33:00.496745Z",
     "iopub.status.busy": "2021-01-14T21:33:00.496020Z",
     "iopub.status.idle": "2021-01-14T21:33:02.110025Z",
     "shell.execute_reply": "2021-01-14T21:33:02.110642Z"
    },
    "papermill": {
     "duration": 1.663463,
     "end_time": "2021-01-14T21:33:02.110804",
     "exception": false,
     "start_time": "2021-01-14T21:33:00.447341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAroAAALICAYAAACQF1PTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdeZxU9ZX38e+RRSKgYlhVFqMkjvqAmlaSuEHUoGQRHYlL4vJoohmjo8HRuDxR1CSPcY2ZmCjiggsucYnoo2OMiSAzbo2DChIGUVQCQhskbogC5/mjbseyqd/t6lpv/frzfr3q1VX33OXc23W6Tt269WtzdwEAAACx2ajeCQAAAADVQKMLAACAKNHoAgAAIEo0ugAAAIgSjS4AAACiRKMLAACAKNHoVpiZTTKzWyuwnr3MbEElcsq6Sh0zoKOo146jXlEP1GrHUas50Te6ZrbYzPZrM+1YM5tVr5yK4e5PuPsXWh+33Q8zG2ZmbmZd65Nh47CcX5jZ35LbJWZm9c4LG6JeYWZnmNlcM3vXzF41szPqnRM2RK2iUV5b+UVmkJl1dfe19c6jEQWO3QmSxksaKcklPSrpFUnX1Dg9RIh6LV3g2JmkoyW9IGlbSX8wszfc/Y6aJ4ioUKula+TX1ujP6BbDzM4ys0XJGYSXzOzgvNixZjbLzC4zs7eTMwwH5sW3MbMZybKPSuqbF5tqZqcn97dK3iWelDzezsxWJu+IRpvZEjP7sZm9KenG1mnJvLdIGiLpATN7z8zOlDQz2cyqZNqXk3mPM7P5Sa6PmNnQvHzczH5gZguT+NWhd1/JRx53mdnNyb7NM7OmNuvaLu/xTWb20+R+6/6caWYrzGyZmY03s3Fm9j/Jfp/TZpM9zOzOZFvPmdnIvHVvaWb3mFlLcvz/tU2ed5vZrWb2jqRjC+zOMZIud/cl7v5XSZcH5kMDoF4LHpNo6tXdL3H359x9rbsvkHS/pD0K7TeyjVoteEyiqVU1yGsrjW7OIkl7SdpM0gWSbjWzQXnxUZIWKFdol0i6Pu9JPE3S7CR2kXK/+FYzJI1O7u+j3DudfZLHe0t6wj/5H8wDJW0haahy75L+wd2PkvS6pG+6ey93vyRZXpI2T6Y9aWbjJZ0j6RBJ/SQ9Ien2Nvv6DUm7KfcO7NuSxqYcl29JukPS5pKmS/p1yrxtDZTUQ9JWks6TdJ2k70r6onLH+jwz+1ze/AdJ+p1yx2CapN+bWTcz20jSA5KeT9a1r6TTzGxsm2XvTvK8rUAuOybLt3o+mYbGRL0WFku9/kPye9tL0rwO7Auyg1otLJZabYzXVneP+iZpsaT3JK3Ku30gaVbKMnMkHZTcP1bSy3mxTZQ7RT9QuXeCayX1zItPk3Rrcn/bZHsbKXcq/0RJS5LYVEkTk/ujJX0kqUfeeka3zpu3H/vlPR6W5NE1b9rDko7Pe7xRsq9Dk8cuac+8+F2Szgocg0mS/pj3eAdJq/Meu6Tt8h7fJOmnebmvltQledw7mX9U3vyzJY3P29ZTbfJeplzRjpL0epvczpZ0Y96yM9t5DqyTtH3e4+FJPlbv5ye3DX5X1Gsnr9c2y16g3IvnxvV+bnLb4HdDrXbyWlWDvLZ2ljO6491989abpJPyg2Z2tJnNMbNVZrZK0k7K+5hE0putd9z9g+RuL0lbSnrb3d/Pm/e1vHkXKfeHYGflnlgPSlpqZl9Q7t3njLzlWtz9wzL3c6ikq/L2Y6Vy17ttVWhflCvUXinraztvDyv+Av2/ufu65P7q5OfyvPjqNtt+o/WOu6+XtES54ztU0pat+5Ts1zmSBhRaNuA9SZvmPd5U0nueVCYyh3otsC/qPPUqSTKzk5W7Vvfr7r6myP1AbVGrBfZFnadWG+K1tdN/GS25zuY65U7bP+nu68xsjnJP4vYsk9THzHrmFeQQ5d7RtJoh6VBJ3d39r2Y2Q7k/3n2Ue3fbqr0nRtt4ofnfkPQzd0/9OLBCPlDuHXirgcoVUKkGt95JPlLZWtJS5d7Vv+ruw1OWbe/YzVPu46RnkscjxUehDYl6LVkj1avM7DhJZ0na293LyRN1Qq2WrJFqtSFeWzvLGd00PZX7ZbZIkpn9b+XedbbL3V+T1CzpAjPrbmZ7Svpmm9lmSDpZn1zg/rikU5T7eGedirdcUv51Ny2S1reZdo2ks81sx2RfNjOzCR3YRkfMkXSkmXUxswP0yfVRpfqimR2SvKs9TdIaSU8pV0DvWO7LBJ9JtreTme3WgXXfLGmi5b60sKWk05X7OAiNh3otTcPUq5l9R9LPJe3v7q+UmSfqh1otTcPUqhrktbXTN7ru/pJy3xR8Urkn/P+S9J8dWMWRyl3rslLS+cr94vPNUO46mtZinKXcu7WZ6pj/K+n/JB8x/FvyMc/PJP1nMu1L7n6fpF9IusNy35KcK+nAlHWW41Tl/vCskvQdSb8vc333SzpM0tuSjpJ0iLt/nPzB+qZyH1G9KuktSVOU+3JDsa5V7qL7F5U7Jv8vmYYGQ72WrJHq9aeSPivpWct96/09M8vUcEVoH7Vaskaq1YZ4bbWMXUoBAAAAVESnP6MLAACAONHoAgAAIEo0ugAAAIgSjS4AAACiVNNxdPv27evDhg2r5SaBTJk9e/Zb7t6v3nkUg3pFZ7Z48WK99dZbxYz5WnfUKjq7tNfWshrdZIy3qyR1kTTF3S9Om3/YsGFqbm4uZ5NAQzOz19qfq2rbpl6BIjU1NdV1+x2pV2oVnV3aa2vJly6YWRdJVys3ltwOko4wsx1KXR+A6qFegcZBvQKVU841urtLetndX3H3jyTdIemgyqQFoMKoV6BxUK9AhZTT6G6l3P9/brUkmfYpZnaCmTWbWXNLS0sZmwNQBuoVaBzt1iu1ChSnnEa30EX6G/ybNXef7O5N7t7Ur19DfAcHiBH1CjSOduuVWgWKU06ju0TS4LzHW0taWl46AKqEegUaB/UKVEg5je6zkoab2TZm1l3S4ZKmVyYtABVGvQKNg3oFKqTk4cXcfa2ZnSzpEeWGP7nB3edVLDMAFUO9Ao2DegUqp6xxdN39IUkPVSgXAFVEvQKNg3oFKoN/AQwAAIAo0egCAAAgSjS6AAAAiBKNLgAAAKJEowsAAIAo0egCAAAgSjS6AAAAiBKNLgAAAKJEowsAAIAo0egCAAAgSjS6AAAAiBKNLgAAAKJEowsAAIAo0egCAAAgSjS6AAAAiBKNLgAAAKJEowsAAIAo0egCAAAgSl3rnQBKs2LFimDsT3/6U8HpEydODC7j7sHYM888E4wNHjw4GAMAIBarVq0qOP2WW24JLhN6PZakU045JRgbM2ZMMGZmwRg2xBldAAAARIlGFwAAAFGi0QUAAECUaHQBAAAQJRpdAAAARIlRFzLsgw8+CMb23HPPYGzRokUFp6d9U7N///7B2KuvvhqMMeoCAKCRrF+/Phh79NFHg7HTTjut4PQFCxaUlMf9998fjKWNknTBBRcEYz179iwpl5iV1eia2WJJ70paJ2mtuzdVIikAlUe9Ao2DegUqoxJndMe4+1sVWA+A6qNegcZBvQJl4hpdAAAARKncRtcl/cHMZpvZCYVmMLMTzKzZzJpbWlrK3ByAMlCvQONIrVdqFShOuY3uHu6+q6QDJf3QzPZuO4O7T3b3Jndv6tevX5mbA1AG6hVoHKn1Sq0CxSmr0XX3pcnPFZLuk7R7JZICUHnUK9A4qFegMkr+MpqZ9ZS0kbu/m9z/mqQLK5ZZJ1HpIcQkyd07nEfaMn/4wx+CsTlz5gRjBx98cDC28cYbB2NpQ52hNNRrZaxcuTIYe+aZZ4KxN998Mxi75pprCk7/8pe/HFzma1/7WjCWNozg6NGjg7EePXoEY6gt6rW63njjjWDswAMPrGEmYVdccUUwttNOOwVjRx99dDC20Uad82tZ5Yy6MEDSfckf1a6Sprn7f1QkKwCVRr0CjYN6BSqk5EbX3V+RNLKCuQCoEuoVaBzUK1A5nfM8NgAAAKJHowsAAIAo0egCAAAgSjS6AAAAiFI5oy6gSGlDiJ177rnB2PPPPx+MpQ0hVMoyK1asCMYuvvjiYCxtWLKJEycGY5tsskkwdvzxxwdjhx9+eDA2atSoYAwo1r/9278FY7/5zW+Csa5dw39ON91002As9HxP+29X3/ve94KxtDp///33g7EDDjggGDv00EODsfHjxwdjXbp0CcaAakp7ro8bN65meXznO98Jxm677baS1nnccccFY/vtt18wtvXWW5e0vUbHGV0AAABEiUYXAAAAUaLRBQAAQJRodAEAABAlGl0AAABEiUYXAAAAUWJ4sRqYPn16MParX/0qGEsbguuXv/xlMDZo0KDiEitS2hAoy5YtK2md8+fPD8bSjskdd9wRjKUdk8MOO6y4xNDprVmzJhjbd999g7Frr702GNtyyy3LyqmttGHO0qQNWThhwoRg7K677grGzj///GAsbfhEhh5DNa1evToYS3v9qbQTTzwxGHv99deDsSeeeKKk7V1xxRUlxWLGGV0AAABEiUYXAAAAUaLRBQAAQJRodAEAABAlGl0AAABEiUYXAAAAUTJ3r9nGmpqavLm5uWbbq6UVK1YEYzvvvHNJy6UNV3T88ccXl1hG/f3vfw/GzjjjjGDs+uuvD8bMLBhbu3ZtcYlVmZnNdvemeudRjJjrNc3KlStLWm6LLbaocCa11dLSEoztsccewdiiRYuCsYceeigYGzt2bHGJ1UlTU5Oam5vDf1QypLPWapq33norGOvfv39FtzVy5MhgbNasWcHYkiVLgrFddtklGPvwww+Dse233z4YS3uOpA1n2gjSXls5owsAAIAo0egCAAAgSjS6AAAAiBKNLgAAAKJEowsAAIAo0egCAAAgSl3bm8HMbpD0DUkr3H2nZNoWku6UNEzSYknfdve3q5dm9k2fPj0Ye/PNN4Oxgw46KBhr9CHE0my22WbBWNp+T5kypRrpRIN6LV+jDxNWqn79+gVjDz74YDC24447BmNf//rXg7E//elPwdjee+8djMWEeq2e3r17B2MHHHBAMPYf//EfHd7WueeeG4z17NkzGEsb0istlja82F/+8pdg7OOPPw7GYlbMGd2bJLV9Vpwl6TF3Hy7pseQxgPq7SdQr0ChuEvUKVFW7ja67z5TUdgT1gyRNTe5PlTS+wnkBKAH1CjQO6hWovlKv0R3g7sskKflZ2X8zAqCSqFegcVCvQAVV/ctoZnaCmTWbWXPav5gEUH/UK9AYqFWgOKU2usvNbJAkJT9XhGZ098nu3uTuTWlfdgBQNdQr0DiKqldqFShOqY3udEnHJPePkXR/ZdIBUAXUK9A4qFeggooZXux2SaMl9TWzJZLOl3SxpLvM7HhJr0uaUM0ks+KDDz4Ixi655JJgzMyCsSOPPLKsnGI0atSoYCztWKbFOgvqFdXw+c9/Phjbd999g7FHH300GFu9enVZOcWAeq2ejz76KBibP39+SescMWJEwekHHnhgSevr06dPMJZ2ln7lyrbfX/zEIYccEoylDVkWs3YbXXc/IhAK/3UDUBfUK9A4qFeg+vjPaAAAAIgSjS4AAACiRKMLAACAKNHoAgAAIEo0ugAAAIhSu6Mu4BM///nPg7FFixYFY/vss08wNmECI8e09dBDDwVj7l7SOlesKPw/Evr3579rAuXYbbfdgrG04cWAalqzZk0w9tprr5W0zkGDBhWc3rNnz5LW16tXr2AsbXixBQsWBGNDhgwJxrp161ZcYpHhjC4AAACiRKMLAACAKNHoAgAAIEo0ugAAAIgSjS4AAACiRKMLAACAKDG8WAfMnTs3GDOzGmbSeaUd57QYw4ihWBdddFEwNm3atIpvb8SIEQWnH3roocFl+vTpE4ztt99+ZefUEc8++2xNtwcUY/ny5fVOoV2rV68Oxt5+++0aZhI3zugCAAAgSjS6AAAAiBKNLgAAAKJEowsAAIAo0egCAAAgSoy60MbChQuDsenTpwdjad/433///cvKCZ9w92DsW9/6Vg0zQa2sW7cuGFu0aFEwdtdddwVjF154YTDWtWv4z+LQoUODsVK98MILHZouSW+++WYwtmrVqmDs1FNPDcYmTJgQjG277bbBWNpoNN27dw/GRo4cGYwB5ZoyZUrF13nSSSdVdH0LFiwIxubNm1fRbXVmnNEFAABAlGh0AQAAECUaXQAAAESJRhcAAABRotEFAABAlGh0AQAAECWGF2sjbXixtCHE0mJpQ/pgQ7fddlswlnacjzzyyGqkgzq79957g7HDDz+8pHWmDXt13333BWNjx44taXuVlja82L//+78HY1dffXVJy33hC18IxpYtWxaM9e7dOxgbOHBgMAYUI+31+vbbby9pnZtvvnkwNmLEiJLWifpq94yumd1gZivMbG7etElm9lczm5PcxlU3TQDFoF6BxkG9AtVXzKULN0k6oMD0K9195+T2UGXTAlCim0S9Ao3iJlGvQFW12+i6+0xJK2uQC4AyUa9A46Begeor58toJ5vZC8lHL31CM5nZCWbWbGbNLS0tZWwOQBmoV6BxtFuv1CpQnFIb3d9K2lbSzpKWSbo8NKO7T3b3Jndv6tevX4mbA1AG6hVoHEXVK7UKFKekRtfdl7v7OndfL+k6SbtXNi0AlUK9Ao2DegUqq6ThxcxskLu3jilzsKS5afM3kssuuywYc/dg7JxzzgnGNtlkk7JyitEdd9xRUmybbbYJxr7+9a+XlVOsGr1e77777pKWGzlyZDD20EPh7/c0wrBXaTn+7Gc/C8ZOP/30YGzChAnB2OOPP15UXm299957wdjTTz8djI0aNaqk7cWg0eu1lqZMmRKMrVixoqR1fve73w3Ghg4dWtI6Q1566aWKrk+Sdtxxx4qvs9G12+ia2e2SRkvqa2ZLJJ0vabSZ7SzJJS2WdGIVcwRQJOoVaBzUK1B97Ta67n5EgcnXVyEXAGWiXoHGQb0C1ce/AAYAAECUaHQBAAAQJRpdAAAARIlGFwAAAFEqaXixzsrM6p1CQ0kb3mXixInBWNpxnjZtWjDGMG5xev/990tabp999gnGBgwYUGo6De2xxx4Lxp555plgrGvX8EvFT37yk2AsbbjGMWPGBGO//vWvC04/7rjjgssAlTBixIiabevmm2+u+DrHjx9f8XU2Os7oAgAAIEo0ugAAAIgSjS4AAACiRKMLAACAKNHoAgAAIEo0ugAAAIgSw4t1gLvXO4XMeeONN4KxUaNGBWNvvvlmMHbVVVcFY7vvvntxiSEaX/rSl4Kxhx9+OBj71a9+FYxdcsklwVi3bt2KS6yO1q5dG4w9++yzwdi//Mu/BGNpQ66lDROWNpzRV7/61WBswoQJwdhFF11UcPry5cuDy5x99tnBGFCsoUOHVnR906dPD8ZmzpxZ0jqHDx8ejPXo0aOkdcaMM7oAAACIEo0uAAAAokSjCwAAgCjR6AIAACBKNLoAAACIEo0uAAAAosTwYh1gZvVOoWo++OCDYOzxxx8Pxr73ve8FYytWrChpuR/84AfBGDqf4447Lhi7/vrrg7HXX389GEsbXmzvvfcuLrEq+81vfhOMpQ3r9+STTwZj22+/fTA2derUYKypqSkYS/OVr3wlGJs7d24w9ve//73g9D59+pSUB1Csa6+9Nhjbf//9O7y+G264IRj78MMPO7w+STrnnHOCsZ49e5a0zphxRhcAAABRotEFAABAlGh0AQAAECUaXQAAAESJRhcAAABRotEFAABAlNodXszMBku6WdJASeslTXb3q8xsC0l3ShomabGkb7v729VLtTYGDRoUjK1fvz4Y+/nPfx6MnX322cHYJptsUlxiHfDRRx8VnJ427NCYMWOCsbRh1dw9GEsbQmzy5MnBGEoXY71uueWWwVjac/p3v/tdMHbBBRcEY5deemkw9vHHHwdjaUMFbbzxxsFY9+7dg7GQCy+8MBg766yzgrG04ZHScqyGtKHCOsMwYjHWaq2NGzcuGHvkkUeCseeffz4Ymz59ejAWGoKvV69ewWUefvjhYKxUY8eOrfg6Y1bMGd21kk5393+S9CVJPzSzHSSdJekxdx8u6bHkMYD6ol6BxkCtAjXQbqPr7svc/bnk/ruS5kvaStJBklrf3kyVNL5aSQIoDvUKNAZqFaiNDl2ja2bDJO0i6WlJA9x9mZQrWEn9A8ucYGbNZtbc0tJSXrYAika9Ao2BWgWqp+hG18x6SbpH0mnu/k6xy7n7ZHdvcvemfv36lZIjgA6iXoHGQK0C1VVUo2tm3ZQrxNvc/d5k8nIzG5TEB0laUZ0UAXQE9Qo0BmoVqL52G13LfeX+eknz3f2KvNB0Scck94+RdH/l0wPQEdQr0BioVaA2LG14KEkysz0lPSHpReWGQJGkc5S7luguSUMkvS5pgruvTFtXU1OTNzc3l5tzVX3wwQfBWO/evYOxtCG49t5772Ds2muvDcaGDx8ejD399NPB2JQpUwpOv/HGG4PLpD0P0vbt1VdfDcYGDhwYjHXr1i0Yi5mZzXb3piquv1PVa62tXBk+ZM8++2wwtuuuuwZjfOycTU1NTWpubg7/8SsTtVpd9957bzB26KGHlrTOLl26FJye9hq5du3akrZ1zTXXBGNpQ3dutFHn/PcIaa+t7Y6j6+6zJIV+i/uWkxiAyqJegcZArQK10TlbfwAAAESPRhcAAABRotEFAABAlGh0AQAAECUaXQAAAESp3VEXOptNNtkkGEsbCmzixInB2J///OdgbPvttw/GSh3yK7Rc2nBfF154YTCWNpQJ0JlsscUWwdjYsWNrmAmANGnDepZq3bp1FV3fEUccEYwxhFjlcLQAAAAQJRpdAAAARIlGFwAAAFGi0QUAAECUaHQBAAAQJRpdAAAARInhxTogbbiPESNGBGNTpkwJxm644YaSctl1112Dscsvv7zg9JEjRwaX2WyzzUrKAwCArPnsZz8bjP39738PxtKGJZszZ06H80gbevS8884LxhhCrHI4kgAAAIgSjS4AAACiRKMLAACAKNHoAgAAIEo0ugAAAIgSoy5UyO67715SbPLkydVIBwCATsvMgrHevXsHY//93/9djXRQR5zRBQAAQJRodAEAABAlGl0AAABEiUYXAAAAUaLRBQAAQJRodAEAABAlGl0AAABEqd1G18wGm9mfzWy+mc0zs1OT6ZPM7K9mNie5jat+ugDSUK9AY6BWgdoo5h9GrJV0urs/Z2a9Jc02s0eT2JXufln10gPQQdQr0BioVaAG2m103X2ZpGXJ/XfNbL6kraqdGICOo16BxkCtArXRoWt0zWyYpF0kPZ1MOtnMXjCzG8ysT2CZE8ys2cyaW1paykoWQPGoV6AxUKtA9RTd6JpZL0n3SDrN3d+R9FtJ20raWbl3pZcXWs7dJ7t7k7s39evXrwIpA2gP9Qo0BmoVqK6iGl0z66ZcId7m7vdKkrsvd/d17r5e0nWSdq9emgCKRb0CjYFaBaqvmFEXTNL1kua7+xV50wflzXawpLmVTw9AR1CvQGOgVoHaKGbUhT0kHSXpRTObk0w7R9IRZrazJJe0WNKJVckQQEdQr0BjoFaBGihm1IVZkqxA6KHKpwOgHNQr0BioVaA2+M9oAAAAiBKNLgAAAKJEowsAAIAo0egCAAAgSjS6AAAAiBKNLgAAAKJEowsAAIAo0egCAAAgSjS6AAAAiBKNLgAAAKJEowsAAIAombvXbmNmLZJeSx72lfRWzTaeLiu5ZCUPKTu5xJbHUHfvV4H1VF1G6zUreUjZySUreUjZyaUSeTRqrUpx/R4qJSu5kMeGqlqvNW10P7Vhs2Z3b6rLxtvISi5ZyUPKTi7kkQ1Z2f+s5CFlJ5es5CFlJ5es5FEvWdn/rOQhZScX8thQtXPh0gUAAABEiUYXAAAAUapnozu5jttuKyu5ZCUPKTu5kEc2ZGX/s5KHlJ1cspKHlJ1cspJHvWRl/7OSh5SdXMhjQ1XNpW7X6AIAAADVxKULAAAAiBKNLgAAAKJUl0bXzA4wswVm9rKZnVWPHJI8FpvZi2Y2x8yaa7ztG8xshZnNzZu2hZk9amYLk5996pTHJDP7a3Jc5pjZuBrkMdjM/mxm881snpmdmkyvxzEJ5VLz45IF1Cv1WiCPTNQrtfppWanVJJe61GtWajUlF+q1xvVa82t0zayLpP+RtL+kJZKelXSEu79U00RyuSyW1OTuNR802cz2lvSepJvdfadk2iWSVrr7xckfqT7u/uM65DFJ0nvuflk1t90mj0GSBrn7c2bWW9JsSeMlHavaH5NQLt9WjY9LvVGv/9g29frpPDJRr9TqJ7JUq0k+i1WHes1KrabkMknUa03rtR5ndHeX9LK7v+LuH0m6Q9JBdcijrtx9pqSVbSYfJGlqcn+qck+AeuRRc+6+zN2fS+6/K2m+pK1Un2MSyqUzol5FvRbIIxP1Sq1+CrWq7NRqSi4119nrtR6N7laS3sh7vET1+8Pkkv5gZrPN7IQ65ZBvgLsvk3JPCEn965jLyWb2QvLRS00+5mllZsMk7SLpadX5mLTJRarjcakT6jWMelV26pVazVStStmq1yzVqkS91rRe69HoWoFp9RrjbA9331XSgZJ+mHzMAOm3kraVtLOkZZIur9WGzayXpHsknebu79Rqu0XmUrfjUkfUa/Z1+nqlViVlq1Yl6jWEeq1xvdaj0V0iaXDe460lLa1DHnL3pcnPFZLuU+6jn3panlzD0noty4p6JOHuy919nbuvl3SdanRczKybck/+29z93mRyXY5JoVzqdVzqjHoNo14zUK/U6j9kplalzNVrJmpVol7rUa/1aHSflTTczLYxs+6SDpc0vdZJmFnP5GJomVlPSV+TNDd9qaqbLumY5P4xku6vRxKtT/zEwarBcTEzk3S9pPnufkVeqObHJJRLPY5LBlCvYdRrneuVWv2UTNSqlMl6zUStStRroTyqfkzcveY3SeOU+3boIknn1imHz0l6PrnNq3Uekm5X7hT9x8q9Ez9e0mclPSZpYfJzizrlcYukFyW9oFwhDKpBHnsq9zHbC5LmJLdxdTomoVxqflyycKNeqdcCeWSiXqnVDY5H3Ws1yaNu9Qb6GlEAACAASURBVJqVWk3JhXqtcb3yL4ABAAAQJf4zGgAAAKJEowsAAIAo0egCAAAgSjS6AAAAiBKNLgAAAKJEowsAAIAo0egCAAAgSjS6AAAAiBKNLgAAAKJEowsAAIAo0egCAAAgSjS6AAAAiBKNboWZ2SQzu7UC69nLzBZUIqesq9QxAzqKeu046hX1QK12HLWaE32ja2aLzWy/NtOONbNZ9cqpGO7+hLt/ofVx2/0ws2Fm5mbWtT4ZNg4z29zMpprZiuQ2qd45oTDqFWZ2mpm9YmbvmNlSM7uS45Y91Coa5bU1+ka3EVFgpQscuyslbSJpmKTdJR1lZv+7lnkhXtRr6QLH7gFJu7r7ppJ2kjRS0r/WNDFEiVotXSO/ttLoSjKzs8xskZm9a2YvmdnBebFjzWyWmV1mZm+b2atmdmBefBszm5Es+6ikvnmxqWZ2enJ/q+Rd4knJ4+3MbKXljDazJWb2YzN7U9KNrdOSeW+RNETSA2b2npmdKWlmsplVybQvJ/MeZ2bzk1wfMbOhefm4mf3AzBYm8avNzALHZJKZ3WVmNyf7Ns/Mmtqsa7u8xzeZ2U+T+637c2byLm+ZmY03s3Fm9j/Jfp/TZpM9zOzOZFvPmdnIvHVvaWb3mFlLcvz/NS82yczuNrNbzewdSccW2J1vSrrE3T9w98WSrpd0XKH9RvZRrwWPSTT16u6L3H1V6yKS1kvaru18yD5qteAxiaZW1SCvrTS6OYsk7SVpM0kXSLrVzAblxUdJWqBcoV0i6fq8J/E0SbOT2EWSjslbboak0cn9fSS9kvyUpL0lPeHunjweKGkLSUMlnZCfnLsfJel1Sd90917ufkmyvCRtnkx70szGSzpH0iGS+kl6QtLtbfb1G5J2U+4sybcljU05Lt+SdIekzSVNl/TrlHnbGiiph6StJJ0n6TpJ35X0ReWO9Xlm9rm8+Q+S9DvljsE0Sb83s25mtpFyZ3ieT9a1r6TTzGxsm2XvTvK8LZCPtbm/Uwf2BdlCvRYWTb2a2ZHJi+tbyu37tR3YF2QHtVpYNLWqRnhtdfeob5IWS3pP0qq82weSZqUsM0fSQcn9YyW9nBfbRJIr92QbImmtpJ558WmSbk3ub5tsbyNJ10g6UdKSJDZV0sTk/mhJH0nqkbee0a3z5u3HfnmPhyV5dM2b9rCk4/Meb5Ts69DksUvaMy9+l6SzAsdgkqQ/5j3eQdLqvMcuabu8xzdJ+mle7qsldUke907mH5U3/2xJ4/O29VSbvJcpV7SjJL3eJrezJd2Yt+zMdp4Dt0q6N8ljO+X++K6p93OTW8HfFfXayeu1zbLDlWtyBtb7ucltg98NtdrJa1UN8traWc7ojnf3zVtvkk7KD5rZ0WY2x8xWmdkq5d6R9M2b5c3WO+7+QXK3l6QtJb3t7u/nzfta3ryLlPtDsLNyT6wHJS01sy8o9+5zRt5yLe7+YZn7OVTSVXn7sVK5d1hbFdoX5Qq1V8r62s7bw4q/xulv7r4uub86+bk8L766zbbfaL3j7uslLVHu+A6VtGXrPiX7dY6kAYWWDfjXZHsLJd2v3DvxJUXuB2qPei2wL+o89foP7r5Q0jxJvyl2GdQUtVpgX9R5arUhXls7/YXZyXU21yl32v5Jd19nZnP06dPxIcsk9TGznnkFOUS5d1itZkg6VFJ3d/+rmc2QdLSkPsq9u22Vv0whbeOF5n9D0s/cPfQRQyV9oNw78FYDVd4TfHDrneQjla0lLVXuXf2r7j48ZdnUY+fuKyV9J2/9P5f0TBm5ok6o15I1TL0W0FW5M3hoINRqyRqmVhvltbWznNFN01O5X2aLJFnuG4NFXWPi7q9JapZ0gZl1N7M9lbs4O98MSSfrkwvcH5d0inIf76xT8ZZLyr/upkW5L2nkT7tG0tlmtmOyL5uZ2YQObKMj5kg60sy6mNkB+uT6qFJ90cwOSd7VniZpjaSnlCuadyz3ZYLPJNvbycx2K3bFZratmX02WfZA5a7T+mmZ+aI+qNfSNFK9fs/M+if3d1Du49THyswXtUetlqaRarUhXls7faPr7i9JulzSk8o94f+XpP/swCqOVO5al5WSzpd0c5v4DOWuX2ktxlnKvVubqY75v5L+T/IRw78lH/P8TNJ/JtO+5O73SfqFpDuSL3LMlXRgyjrLcapyf3hWKfeO7vdlru9+SYdJelvSUZIOcfePkz9Y31TuI6pXlftyyhTlvtxQrC9KelHSu8odx++4+7wy80UdUK8la6R63UPSi2b2vqSHklvbb5Ij46jVkjVSrTbEa6slFxQDAAAAUen0Z3QBAAAQJxpdAAAARIlGFwAAAFGi0QUAAECUyhpHNxn64ipJXSRNcfeL0+bv27evDxs2rJxNAg1t9uzZb7l7v3psm3oFird48WK99dZbxYz5WhUdqVdqFZ1d2mtryY2umXWRdLWk/ZUbzPhZM5ueDClS0LBhw9Tc3FzqJoGGZ2avtT9XVbZLvQId0NTUVLdtd7ReqVV0dmmvreVcurC7cv+n+hV3/0jSHZIOKmN9AKqHegUaB/UKVEg5je5W+vT/QV6iT//fZ0mSmZ1gZs1m1tzS0lLG5gCUgXoFGke79UqtAsUpp9EtdO3SBv99wt0nu3uTuzf161eXSxMBUK9AI2m3XqlVoDjlNLpLJA3Oe7y1pKXlpQOgSqhXoHFQr0CFlNPoPitpuJltY2bdJR0uaXpl0gJQYdQr0DioV6BCSh51wd3XmtnJkh5RbviTG9x9XsUyA1Ax1CvQOKhXoHLKGkfX3R+S9FCFcgFQRdQr0DioV6Ay+M9oAAAAiBKNLgAAAKJEowsAAIAo0egCAAAgSjS6AAAAiBKNLgAAAKJEowsAAIAo0egCAAAgSjS6AAAAiBKNLgAAAKJEowsAAIAo0egCAAAgSjS6AAAAiBKNLgAAAKJEowsAAIAo0egCAAAgSjS6AAAAiBKNLgAAAKLUtd4JAECWzJkzJxjbddddg7EFCxYEY8OHDy8rJwBAaTijCwAAgCjR6AIAACBKNLoAAACIEo0uAAAAokSjCwAAgCgx6gIA5HnqqadKWu7BBx8MxgYOHBiMvfzyywWnL1q0KLjMihUrgrHPfOYzwdgvf/nLYGzw4MHBGAA0qrIaXTNbLOldSeskrXX3pkokBaDyqFegcVCvQGVU4ozuGHd/qwLrAVB91CvQOKhXoExcowsAAIAoldvouqQ/mNlsMzuhEgkBqBrqFWgc1CtQAeVeurCHuy81s/6SHjWzv7j7zPwZkgI9QZKGDBlS5uYAlIF6BRpHar1Sq0Bxyjqj6+5Lk58rJN0nafcC80x29yZ3b+rXr185mwNQBuoVaBzt1Su1ChSn5DO6ZtZT0kbu/m5y/2uSLqxYZgAqhnotXtrwXGlOP/30CmdSeXPmzAnG5s+fH4x17969GukggHqtrtWrVwdjCxYsCMbOP//8gtObm5uDy3z/+98Pxk466aRgrH///sEYOqacSxcGSLrPzFrXM83d/6MiWQGoNOoVaBzUK1AhJTe67v6KpJEVzAVAlVCvQOOgXoHKYXgxAAAARIlGFwAAAFGi0QUAAECUaHQBAAAQpXL/YQQyKDR0yp133hlc5uabbw7GHn/88WDs0ksvDcZOPfXUYKxrV556yKbDDjssGHvggQeCsfvvvz8Yc/dgrG/fvh1eZr/99gvGBg4cGIxdeeWVwdgLL7wQjDU1NQVjQBYtXLgwGBs7dmww9tprr3V4W2m1etFFFwVjV111VTD2xhtvBGO9evUqLjFI4owuAAAAIkWjCwAAgCjR6AIAACBKNLoAAACIEo0uAAAAokSjCwAAgCgxxlOGrVu3Lhh77rnngrEzzzyz4PSZM2eWlIeZBWNnnHFGMJY2TMvll18ejHXr1q24xIAq6NGjRzB29913B2NpwxmtXbs2GPunf/qn4hIr0ty5c4OxtOHFnnrqqWCM4cWQRaGhNCXpRz/6UTCW9tq02WabBWOHHHJIwelptZ82bN8777wTjN13333B2FFHHRWMYUOc0QUAAECUaHQBAAAQJRpdAAAARIlGFwAAAFGi0QUAAECUaHQBAAAQJYYXy7A//vGPwdi4ceOCMXcvOH3zzTcPLpM2FMvxxx8fjG299dbB2NVXXx2Mffvb3w7G9txzz2AMyKrhw4fXbFuzZs0Kxr7xjW+UtM699tqr1HSAupg+fXow9vDDDwdju+22WzB2yy23BGOhGk8bPjBtCM5f/epXwdiqVauCMXQMZ3QBAAAQJRpdAAAARIlGFwAAAFGi0QUAAECUaHQBAAAQJRpdAAAARInhxerspZdeCsbOOuusktY5evTogtPvu+++4DKbbbZZMPb222+XlEeaaqwTqKdLL700GLvssss6vL7169cHY2n1k7bc4YcfHozttNNOxSUGZMRzzz1X0nKHHnpoMFbKMIFdu4ZbqV122aXD65OkqVOnBmOnnHJKSevsrNo9o2tmN5jZCjObmzdtCzN71MwWJj/7VDdNAMWgXoHGQb0C1VfMpQs3STqgzbSzJD3m7sMlPZY8BlB/N4l6BRrFTaJegapqt9F195mSVraZfJCk1vPqUyWNr3BeAEpAvQKNg3oFqq/UL6MNcPdlkpT87B+a0cxOMLNmM2tuaWkpcXMAykC9Ao2jqHqlVoHiVH3UBXef7O5N7t7Ur1+/am8OQBmoV6AxUKtAcUptdJeb2SBJSn6uqFxKACqMegUaB/UKVFCpw4tNl3SMpIuTn/dXLKNO5sEHHwzGnn/++WBsyJAhwdg999xTcHqPHj2Cy9x5553B2K233hqMpdl0002DsT333LOkdaIk1GuFrF27Nhi77rrrgrGsfLQ8Y8aMYGzp0qXB2ODBg6uRDgqjXot0ySWXBGNmFoztuOOOwdjHH38cjL3++usFp1944YXBZW655ZZgLE3aMIHomGKGF7td0pOSvmBmS8zseOUKcH8zWyhp/+QxgDqjXoHGQb0C1dfuGV13PyIQ2rfCuQAoE/UKNA7qFag+/gUwAAAAokSjCwAAgCjR6AIAACBKNLoAAACIUqnDi6FC0oZASYsNGDAgGHvjjTcKTj/vvPOCyzzwwAPBWKnOPvvsYKxPnz4V3x5QbWnDi3300UclrfPLX/5ywelpQyddeeWVwdj994dHo1q2bFkwduyxxwZjd9xxRzDGPytAvXzlK18Jxp566qlg7MgjjwzGBg0aFIwtWLCg4HR3Dy6T9jqeZueddy5pOWyIM7oAAACIEo0uAAAAokSjCwAAgCjR6AIAACBKNLoAAACIEo0uAAAAosTwYnW22267BWNdu4Z/Pc3NzcFYaFiSUoc5KdUPf/jDmm4PqKfp06eXtNx2221XcPomm2wSXGaPPfYIxhYuXBiMXXDBBcHY7Nmzg7EePXoEY0C9TJw4MRj73ve+F4y98847JcVC+vfvH4ytWbOmpG3dd999wdjFF19cUi6dFWd0AQAAECUaXQAAAESJRhcAAABRotEFAABAlGh0AQAAECVGXaiz0aNHB2NHHXVUMHbjjTcGY0OGDCk4fcyYMcFlbr755mAsTVr+fFMbsUl7To8YMaKGmYQNHz48GNtxxx2DsWnTpgVjl156aTB24YUXFpcYUGH//M//HIyNHTs2GEsb0eCzn/1sMBZ6DU0b0Wjt2rXB2L777huMpY2sNGPGjGBswoQJwVhnxRldAAAARIlGFwAAAFGi0QUAAECUaHQBAAAQJRpdAAAARIlGFwAAAFFieLEMmzJlSkmxkKVLlwZjU6dO7fD6JOnXv/51MNalS5eS1gmgOg477LBg7Nxzzw3GXnnllWqkA1RNr169grG0oTtraaeddgrG0oYXmz17djDG8GIbaveMrpndYGYrzGxu3rRJZvZXM5uT3MZVN00AxaBegcZBvQLVV8ylCzdJOqDA9Cvdfefk9lBl0wJQoptEvQKN4iZRr0BVtdvouvtMSStrkAuAMlGvQOOgXoHqK+fLaCeb2QvJRy99QjOZ2Qlm1mxmzS0tLWVsDkAZqFegcbRbr9QqUJxSG93fStpW0s6Slkm6PDSju0929yZ3b+rXr1+JmwNQBuoVaBxF1Su1ChSnpEbX3Ze7+zp3Xy/pOkm7VzYtAJVCvQKNg3oFKquk4cXMbJC7L0seHixpbtr8yIZ33303GDOzYKxv377B2A477FBWTqg+6nVDa9asCcYmTZoUjA0YMCAY6969e0m57LfffgWnDx06NLjMxhtvXNK2Bg8eHIylDXWE2qFeOw93LymGjmm30TWz2yWNltTXzJZIOl/SaDPbWZJLWizpxCrmCKBI1CvQOKhXoPrabXTd/YgCk6+vQi4AykS9Ao2DegWqj38BDAAAgCjR6AIAACBKNLoAAACIEo0uAAAAolTS8GJoTJdfHvw/AanGjBlT4UyA6ksbnmfixInB2G9+85tgLG0Yvkrbfvvtg7Gtt946GDvxxPCX9Pfdd99gbOnSpcHYiBEjgjEApUn7e5IW+8xnPlONdKLFGV0AAABEiUYXAAAAUaLRBQAAQJRodAEAABAlGl0AAABEiUYXAAAAUWJ4MbTrJz/5Sb1TADrsww8/DMZ++9vfBmNpw/qkDc+14447FpdYG7///e8LTv/LX/4SXCYt9sc//rGkPABU3po1a4KxUmv1qKOOKjWdTokzugAAAIgSjS4AAACiRKMLAACAKNHoAgAAIEo0ugAAAIgSjS4AAACixPBiEVq2bFnB6dOmTQsus9VWWwVjpQ6bBNTTqlWrSlrupZdeCsa23377UtMJ+uUvf1lw+qJFi4LLHHzwwcHY3Llzy86praVLlwZj69atC8a6dOlS8VyARjJ9+vRgbMmSJcHYoEGDgrF+/fqVlVNnwxldAAAARIlGFwAAAFGi0QUAAECUaHQBAAAQJRpdAAAARIlGFwAAAFFieLEIPfLIIwWnr169usaZAPWzcOHCkpYbMmRIhTMpTdrQXAMGDAjGqjG82CuvvBKMMbwYEJY2rGear33ta8FY7969S02nU2r3jK6ZDTazP5vZfDObZ2anJtO3MLNHzWxh8rNP9dMFkIZ6BRoDtQrURjGXLqyVdLq7/5OkL0n6oZntIOksSY+5+3BJjyWPAdQX9Qo0BmoVqIF2G113X+buzyX335U0X9JWkg6SNDWZbaqk8dVKEkBxqFegMVCrQG106MtoZjZM0i6SnpY0wN2XSbmCldQ/sMwJZtZsZs0tLS3lZQugaNQr0BioVaB6im50zayXpHsknebu7xS7nLtPdvcmd2/i/zMDtUG9Ao2BWgWqq6hG18y6KVeIt7n7vcnk5WY2KIkPkrSiOikC6AjqFWgM1CpQfe0OL2ZmJul6SfPd/Yq80HRJx0i6OPl5f1UyREEff/xxMHbGGWd0eH3f//73y0kHGUG9fmL33XcPxnr16hWMDR8+PBhLq63u3bsHY2vWrAnGbrjhhoLT582bF1ymVFtuuWUwdvjhhwdjoSELJSn3lENHUavx+Nvf/haMzZw5Mxhz92BszJgxZeWETxQzju4eko6S9KKZzUmmnaNcEd5lZsdLel3ShOqkCKADqFegMVCrQA202+i6+yxJobfs+1Y2HQDloF6BxkCtArXBvwAGAABAlGh0AQAAECUaXQAAAESJRhcAAABRKmbUBWTQlClTgrHQUCdpg4qfeuqpZecEZEmPHj2CsVmzZgVjo0aNCsYmTpxYVk6VkjY82plnnhmMnXLKKcHYZpttFoz9+Mc/Dsa6desWjAGdwX/9138FY6tWrQrG0v5GffOb3ywrJ3yCM7oAAACIEo0uAAAAokSjCwAAgCjR6AIAACBKNLoAAACIEo0uAAAAosTwYhm2du3aYOyuu+4Kxty94PThw4cHl9l0002LTwxocCNGjAjGFi1aFIxdddVVwdjkyZODsW222SYY23vvvQtO/+pXvxpcZp999gnG0oYJK1Xa0IRAZ/ejH/2opOXOOeecYGzzzTcvNR20wRldAAAARIlGFwAAAFGi0QUAAECUaHQBAAAQJRpdAAAARIlRFzLsww8/DMZmzpwZjJlZwel77LFH2TkBsdtyyy2DsV/84hclxQDE65VXXgnGQq/HknTggQdWIx20wRldAAAARIlGFwAAAFGi0QUAAECUaHQBAAAQJRpdAAAARIlGFwAAAFFieLEM69atWzD2+c9/PhhbsGBBwekjR44sOycAAFCcfffdNxjbYYcdaphJ59XuGV0zG2xmfzaz+WY2z8xOTaZPMrO/mtmc5Dau+ukCSEO9Ao2BWgVqo5gzumslne7uz5lZb0mzzezRJHalu19WvfQAdBD1CjQGahWogXYbXXdfJmlZcv9dM5svaatqJwag46hXoDFQq0BtdOjLaGY2TNIukp5OJp1sZi+Y2Q1m1iewzAlm1mxmzS0tLWUlC6B41CvQGKhVoHqKbnTNrJekeySd5u7vSPqtpG0l7azcu9LLCy3n7pPdvcndm/r161eBlAG0h3oFGgO1ClRXUY2umXVTrhBvc/d7Jcndl7v7OndfL+k6SbtXL00AxaJegcZArQLV1+41umZmkq6XNN/dr8ibPii5xkiSDpY0tzopdl4bb7xxMHb22WcHY+edd17B6fvvv3/ZOSHbqFegMVCr8Vi/fn29U0CKYkZd2EPSUZJeNLM5ybRzJB1hZjtLckmLJZ1YlQwBdAT1CjQGahWogWJGXZglyQqEHqp8OgDKQb0CjYFaBWqDfwEMAACAKNHoAgAAIEo0ugAAAIgSjS4AAACiVMyoC8igo48+uqQYAABAZ8EZXQAAAESJRhcAAABRotEFAABAlGh0AQAAECUaXQAAAESJRhcAAABRMnev3cbMWiS9ljzsK+mtmm08XVZyyUoeUnZyiS2Poe7erwLrqbqM1mtW8pCyk0tW8pCyk0sl8mjUWpXi+j1USlZyIY8NVbVea9rofmrDZs3u3lSXjbeRlVyykoeUnVzIIxuysv9ZyUPKTi5ZyUPKTi5ZyaNesrL/WclDyk4u5LGhaufCpQsAAACIEo0uAAAAolTPRndyHbfdVlZyyUoeUnZyIY9syMr+ZyUPKTu5ZCUPKTu5ZCWPesnK/mclDyk7uZDHhqqaS92u0QUAAACqiUsXAAAAECUaXQAAAESpLo2umR1gZgvM7GUzO6seOSR5LDazF81sjpk113jbN5jZCjObmzdtCzN71MwWJj/71CmPSWb21+S4zDGzcTXIY7CZ/dnM5pvZPDM7NZlej2MSyqXmxyULqFfqtUAemahXavXTslKrSS51qdes1GpKLtRrjeu15tfomlkXSf8jaX9JSyQ9K+kId3+pponkclksqcndaz5ospntLek9STe7+07JtEskrXT3i5M/Un3c/cd1yGOSpPfc/bJqbrtNHoMkDXL358yst6TZksZLOla1PyahXL6tGh+XeqNe/7Ft6vXTeWSiXqnVT2SpVpN8FqsO9ZqVWk3JZZKo15rWaz3O6O4u6WV3f8XdP5J0h6SD6pBHXbn7TEkr20w+SNLU5P5U5Z4A9cij5tx9mbs/l9x/V9J8SVupPscklEtnRL2Kei2QRybqlVr9FGpV2anVlFxqrrPXaz0a3a0kvZH3eInq94fJJf3BzGab2Ql1yiHfAHdfJuWeEJL61zGXk83sheSjl5p8zNPKzIZJ2kXS06rzMWmTi1TH41In1GsY9ars1Cu1mqlalbJVr1mqVYl6rWm91qPRtQLT6jXG2R7uvqukAyX9MPmYAdJvJW0raWdJyyRdXqsNm1kvSfdIOs3d36nVdovMpW7HpY6o1+zr9PVKrUrKVq1K1GsI9Vrjeq1Ho7tE0uC8x1tLWlqHPOTuS5OfKyTdp9xHP/W0PLmGpfValhX1SMLdl7v7OndfL+k61ei4mFk35Z78t7n7vcnkuhyTQrnU67jUGfUaRr1moF6p1X/ITK1KmavXTNSqRL3Wo17r0eg+K2m4mW1jZt0lHS5peq2TMLOeycXQMrOekr4maW76UlU3XdIxyf1jJN1fjyRan/iJg1WD42JmJul6SfPd/Yq8UM2PSSiXehyXDKBew6jXOtcrtfopmahVKZP1molalajXQnlU/Zi4e81vksYp9+3QRZLOrVMOn5P0fHKbV+s8JN2u3Cn6j5V7J368pM9KekzSwuTnFnXK4xZJL0p6QblCGFSDPPZU7mO2FyTNSW7j6nRMQrnU/Lhk4Ua9Uq8F8shEvVKrGxyPutdqkkfd6jUrtZqSC/Va43rlXwADAAAgSvxnNAAAAESJRhcAAABRotEFAABAlGh0AQAAECUaXQAAAESJRhcAAABRotEFAABAlGh0AQAAECUaXQAAAESJRhcAAABRotEFAABAlGh0AQAAECUa3Qozs0lmdmsF1rOXmS2oRE5ZV6ljBnQU9dpx1CvqgVrtOGo1J/pG18wWm9l+baYda2az6pVTMdz9CXf/QuvjtvthZsPMzM2sa30ybBxmNsbM/mxmfzezxfXOB2HUK8zsDDOba2bvmtmrZnZGvXPChqhVNEqtRt/oNiIKrHSBY/e+pBskZbII0dio19IFjp1JOlpSH0kHSDrZzA6vaWKIErVaukauVRpdSWZ2lpktSt6VvGRmB+fFjjWzWWZ2mZm9nbxrOTAvvo2ZzUiWfVRS37zYVDM7Pbm/VfIu8aTk8XZmttJyRpvZEjP7sZm9KenG1mnJvLdIGiLpATN7z8zOlDQz2cyqZNqXk3mPM7P5Sa6PmNnQvHzczH5gZguT+NVmZoFjMsnM7jKzm5N9m2dmTW3WtV3e45vM7KfJ/db9OdPMVpjZMjMbb2bjzOx/kv0+p80me5jZncm2njOzkXnr3tLM7jGzluT4/2ubPO82s1vN7B1Jx7bdF3d/xt1vkfRKoX1FY6FeCx6TmOr1End/zt3XuvsCSfdL2qPQfiPbqNWCx4RarTEa3ZxFkvaStJmkCyTdamaD8uKjJC1Q6hgn6QAAG69JREFUrtAukXR93pN4mqTZSewiScfkLTdD0ujk/j7KNVr7JI/3lvSEu3vyeKCkLSQNlXRCfnLufpSk1yV90917ufslyfKStHky7UkzGy/pHEmHSOon6QlJt7fZ129I2k3SSEnfljQ25bh8S9IdkjaXNF3Sr1PmbWugpB6StpJ0nqTrJH1X0heVO9bnmdnn8uY/SNLvlDsG0yT93sy6mdlGkh6Q9Hyyrn0lnWZmY9sse3eS520dyBGNiXotLLp6TX5ve0ma14F9QXZQq4VRq7Xk7lHfJC2W9J6kVXm3DyTNSllmjqSDkvvHSno5L7aJJFfuyTZE0lpJPfPi0yTdmtzfNtneRpKukXSipCVJbKqkicn90ZI+ktQjbz2jW+fN24/98h4PS/LomjftYUnH5z3eKNnXocljl7RnXvwuSWcFjsEkSX/Me7yDpNV5j13SdnmPb5L007zcV0vqkjzuncw/Km/+2ZLG523rqTZ5L1OuaEZJer1NbmdLujFv2ZlFPhf2k7S43s9Jbqm/I+qVes1f9gLlXog3rvdzk9sGvxtqlVrNXzaztdpZzuiOd/fNW2+STsoPmtnRZjbHzFaZ2SpJOynvYxJJb7becfcPkru9JG0p6W13fz9v3v/f3r0HSVWeeRz/PSJEMxgRGUAJG1xirZcYIA7EipaFZTTGS0xkDWpiUFOrKYVgSo2XtYxaq0ETNUqtSeEliAVRU0S0LHURcbVIojICi6LlfQgokXENwcQoMjz7RzfrOPbb09N9+lze+X6qpqbnPN3nPN3Mb3im+8zba7td91WVfhCMV+kb6wFJb5rZv6j02+fj3W7X6e7vN3g/Pyfpxm734x2VzqEZVem+qBTUwVX21/O6O1nt5zj9r7t3lS//o/z5rW71f/Q49rrtF9x9m6T1Kj2+n5O05/b7VL5fl0gaUem2iAJ5rXBf1M/yambTVTr/7xh3/6DG+4F0kdUK90VkNVf6/YnZ5fNsblHpafs/unuXma1S6Zu4Nxsk7WZmLd0C+U8q/Ya13eOS/lXSIHd/w8we10cnb6/qdr3ut6mkZ73S9ddJusrd03j5/j2VfgPfbqRKAarX6O0Xyi+pfFbSmyr9Vv+6u+9d5ba9PXaIBHmtW6HyamZnSLpI0qHu3kifyAhZrRtZTVh/eUa3mhaV/jE7JcnMTlfpt85euftaSe2SrjCzQWZ2iKTjelztcUnT9dEJ7v8taYZKL+90qXZvSep+3k2npG09tv1K0sVmtn/5vuxqZif24Rh9sUrSKWY2wMyO0kfnR9XrQDM7ofxb7bmSPpD0pKSnJW220h8T7Fw+3hfMbGKtOzazHcxsJ0kDS1/aTmY2qMF+kQ3yWp8i5fU7kq6WdIS78wekxUVW60NWE9bvB113f17SdZL+qNI3/AGSft+HXZyi0rku70j6iaR5PeqPq3QezfYwLlPpt7Un1Dc/lXRp+SWG88sv81wl6fflbQe5+72SrpF0l5X+SvI5SV+vss9GzFTpB88mSd+RtKjB/d0naaqkv0g6VdIJ7v5h+QfWcSq9RPW6pLcl3arSHzfU6lCVXs55UKVnBf4haXGD/SID5LVuRcrrf0jaXdJyK/3V+9/M7FcN9ouUkdW6kdWEWfkkYgAAACAq/f4ZXQAAAMSJQRcAAABRYtAFAABAlBh0AQAAEKWG1tEtL31xo6QBkm5191nVrj9s2DAfM2ZMI4cECu2ZZ555291bszg2eQVq19HRobfffruWNV+boi95Javo76r931r3oGtmAyT9p6QjVFrMeLmZ3V9eUqSiMWPGqL29vd5DAoVnZmt7v1ZTjktegT5oa2vL7Nh9zStZRX9X7f/WRk5dmKTS+1S/5u5bJN0l6fgG9gegecgrUBzkFUhII4PuKH38fZDX6+Pv+wwgP8grUBzkFUhII4NupXOXPvHuE2Z2ppm1m1l7Z2dnA4cD0ADyChRHr3klq0BtGhl010sa3e3rz0p6s+eV3H2Ou7e5e1trayZ/gwOAvAJF0mteySpQm0YG3eWS9jazvcxskKSTJN2fTFsAEkZegeIgr0BC6l51wd23mtl0Sf+l0vInt7v7msQ6A5AY8goUB3kFktPQOrru/qCkBxPqBUATkVegOMgrkAzeGQ0AAABRYtAFAABAlBh0AQAAECUGXQAAAESJQRcAAABRYtAFAABAlBh0AQAAECUGXQAAAESJQRcAAABRYtAFAABAlBh0AQAAECUGXQAAAESJQRcAAABRYtAFAABAlBh0AQAAECUGXQAAAESJQRcAAABRYtAFAABAlBh0AQAAEKUds24AAPqzTZs2Vdx+zTXXBG8za9asYG3UqFHB2ooVK4K14cOHB2sAwrq6uoK1V199NVi755576jretGnTgrXRo0fXtc+Y8YwuAAAAosSgCwAAgCgx6AIAACBKDLoAAACIEoMuAAAAosSqCwDQZEuXLg3Wpk+fXnH7iy++GLzNDjuEn6PYsGFDsDZx4sRg7emnnw7WRowYEawBRbNly5ZgrdoqCStXrqy4fdGiRcHbLFy4sPbGajR58uRgjVUXPqmhQdfMOiS9K6lL0lZ3b0uiKQDJI69AcZBXIBlJPKN7mLu/ncB+ADQfeQWKg7wCDeIcXQAAAESp0UHXJS02s2fM7MxKVzCzM82s3czaOzs7GzwcgAaQV6A4quaVrAK1aXTQPdjdvyTp65LOMbNDe17B3ee4e5u7t7W2tjZ4OAANIK9AcVTNK1kFatPQoOvub5Y/b5R0r6RJSTQFIHnkFSgO8goko+4/RjOzFkk7uPu75ctHSroysc5Qt82bN1fcPmTIkOBt3D1Y22uvvYK1p556KljjWYb8IK/Nd9NNNwVr5513XrC2bdu2ZrRT0fr164O1asuZsbxYushr45YsWRKs/ehHPwrWnn/++T4fq9r/n2bW5/315oorrgjWZs+eHazts88+ifdSBI2sujBC0r3lf8QdJS1w94cT6QpA0sgrUBzkFUhI3YOuu78maVyCvQBoEvIKFAd5BZLD8mIAAACIEoMuAAAAosSgCwAAgCgx6AIAACBKjay6gIKptsxJtdratWuDtalTpwZrCxYsCNZGjhwZrAF51d7eHqydf/75wVrSS4gdccQRwdojjzxS1z4vuOCCYK3aMoJAVpYuXRqsfe1rX0uxk3RVu99f/OIXg7UpU6YEa/PmzQvWBg4cWFtjOcUzugAAAIgSgy4AAACixKALAACAKDHoAgAAIEoMugAAAIgSgy4AAACixPJiaMjjjz8erHV0dARrLC+GvPrDH/4QrP3gBz8I1rq6uuo63ujRoytur7aE0NChQ4O1r371q8HaypUrg7XNmzcHa0AezZ8/P+sWcqfaz6F77rknWDvttNOCtaIv1cYzugAAAIgSgy4AAACixKALAACAKDHoAgAAIEoMugAAAIgSgy4AAACixPJiANDN6aefHqy98sorde3zyiuvDNZmzpxZcfvgwYODt1m7dm2wVm0JMSAm1ZbEmjt3bmp9VPPDH/4wWDv88MODtWpLdy5ZsiRYW716dW2N9VDt8WJ5MQAAACCHGHQBAAAQJQZdAAAARIlBFwAAAFFi0AUAAECUGHQBAAAQpV6XFzOz2yUdK2mju3+hvG2opLsljZHUIenb7v6X5rWJvDr22GODtb333jvFTiCR1yydeOKJwdqFF14YrO24Y99Xebzhhhv6fBvkD3ltzNixY4O1X/ziF8Haxo0bg7X33nsvWAtl/KWXXgre5pRTTgnWqmX/yCOPDNauuuqqYO2BBx4I1qr9jPr85z8frBVdLc/ozpV0VI9tF0l61N33lvRo+WsA2Zsr8goUxVyRV6Cpeh103f0JSe/02Hy8pDvKl++Q9M2E+wJQB/IKFAd5BZqv3nN0R7j7Bkkqfx4euqKZnWlm7WbW3tnZWefhADSAvALFUVNeySpQm6b/MZq7z3H3Nndva21tbfbhADSAvALFQFaB2tQ76L5lZntIUvlz+MxuAFkjr0BxkFcgQfUOuvdLmla+PE3Sfcm0A6AJyCtQHOQVSFAty4v9RtJkScPMbL2kn0iaJekeM/u+pD9JCq9ZgcIbMGBAsHbIIYcEa7vvvnsz2kEV5LVxy5YtC9aWLl0arB11VM8/nv9IPUuIVdOMczIvu+yyxPeJ6shrY/bcc89gbcaMGan1cdBBByW+z0GDBtV1uzlz5gRr1X4OfeMb36jreEXQ609fdz85UDo84V4ANIi8AsVBXoHm453RAAAAECUGXQAAAESJQRcAAABRYtAFAABAlBh0AQAAEKVk17xBlPbff/9g7YILLkixE6D5qr3L1NSpU1PsJF2jRo3KugUANVi3bl2w9uc//zlY27p1a7C2fPnyYG3ixIm1NZZTPKMLAACAKDHoAgAAIEoMugAAAIgSgy4AAACixKALAACAKDHoAgAAIEosLxahm2++OesWADSo2nI/ixcvTrETAHny2GOPBWvPPvtsXfscO3Zsve3kHs/oAgAAIEoMugAAAIgSgy4AAACixKALAACAKDHoAgAAIEqsuhChBQsWJLq/RYsWJbo/AB95//33K27/8Y9/HLzNO++8U9exjj322GDtwAMPrGufAJL38MMPB2unnXZaXfscMWJEsDZu3Li69lkEPKMLAACAKDHoAgAAIEoMugAAAIgSgy4AAACixKALAACAKDHoAgAAIEosL1ZQr732WrC2adOmRI81dOjQRPcH9DehJcQkaebMmRW3P/HEE3Ud69Of/nSwNmvWrGCtpaWlruMBebRt27ZgrdryfJ2dncHaypUrK25fvHhx8DZmFqxVM3/+/MT3eckllwRrI0eOrGufRdDrM7pmdruZbTSz57ptu9zM3jCzVeWPo5vbJoBakFegOMgr0Hy1nLowV9JRFbbf4O7jyx8PJtsWgDrNFXkFimKuyCvQVL0Ouu7+hKT63oYHQKrIK1Ac5BVovkb+GG26ma0uv/SyW+hKZnammbWbWXu1c18ANBV5BYqj17ySVaA29Q66v5Q0VtJ4SRskXRe6orvPcfc2d29rbW2t83AAGkBegeKoKa9kFahNXYOuu7/l7l3uvk3SLZImJdsWgKSQV6A4yCuQrLqWFzOzPdx9Q/nLb0l6rtr1kbxf//rXwdobb7yRYifIO/KavYsvvjhYu/XWWxM91nHHHRes7bvvvokeC8nrj3l99NFHg7W77747WFu9enWw1tXVFaytW7cuWKvnNBB3D9bqXQqsGa67Lvhink466aRgreivGPQ66JrZbyRNljTMzNZL+omkyWY2XpJL6pB0VhN7BFAj8goUB3kFmq/XQdfdT66w+bYm9AKgQeQVKA7yCjQfbwEMAACAKDHoAgAAIEoMugAAAIgSgy4AAACiVNfyYkjHBx98EKxt2rQp0WNVW8KlpaUl0WMBRVUtk9WWOlqwYEGifey8887B2mWXXZbosYCetm3bVnH7+vXrg7c57LDDgrWOjo5GW8pUUZYXq7as2sSJE4O1avPBl7/85YZ6SgPP6AIAACBKDLoAAACIEoMuAAAAosSgCwAAgCgx6AIAACBKDLoAAACIEsuL5diKFSuCtZtvvjnRYw0bNixY22EHfh9C/7Fly5Zg7aGHHgrWpkyZ0ox2Kho3blywNnbs2NT6QP90xhlnVNx+5513ptxJPlRbQmzIkCHB2qRJk4K14cOHB2srV66srbEe1qxZE6xVW3ps0aJFwdqBBx5YcfuOO+ZnvGSCAQAAQJQYdAEAABAlBl0AAABEiUEXAAAAUWLQBQAAQJQYdAEAABCl/Kz/gKY74YQTgrUJEyak2AmQrZdffjlYu+iii4K1asvspOnJJ58M1k499dRg7dJLLw3W9tprr2CtpaWltsbQL8ybN6/i9mrLbMVg2rRpFbcPHjw4eJvQUmySNH78+GBt8+bNwdrAgQODtWp++tOfBmtXXXVVsHbttdcGa6ecckrF7QcccEDtjTUZz+gCAAAgSgy6AAAAiBKDLgAAAKLEoAsAAIAoMegCAAAgSgy6AAAAiFKvy4uZ2WhJ8ySNlLRN0hx3v9HMhkq6W9IYSR2Svu3uf2leq6jVpz71qYrbJ0+eHLzNrrvu2qRukCby+pEPP/wwWLv66quDtbwsIVav3/72t3XVqv0MWLZsWbC233771dYYPqbIWT388MMrbl+6dGnKnSRv5513Dtba2toqbj/77LMT7+Mzn/lM4vustrzguHHjgrXvfe97wdpXvvKVitvffffd2htrslqe0d0q6Tx331fSQZLOMbP9JF0k6VF331vSo+WvAWSLvALFQFaBFPQ66Lr7BndfUb78rqQXJI2SdLykO8pXu0PSN5vVJIDakFegGMgqkI4+naNrZmMkTZD0lKQR7r5BKgVW0vDAbc40s3Yza+/s7GysWwA1I69AMZBVoHlqHnTNbLCkhZLOdffwe9P14O5z3L3N3dtaW1vr6RFAH5FXoBjIKtBcNQ26ZjZQpSDOd/fflTe/ZWZ7lOt7SNrYnBYB9AV5BYqBrALN1+uga2Ym6TZJL7j79d1K90uaVr48TdJ9ybcHoC/IK1AMZBVIR6/Li0k6WNKpkp41s1XlbZdImiXpHjP7vqQ/STqxOS2ir/bcc8+K288555yUO0EGyGvZQw89FKzNmzcvxU6klpaWYG3q1KkVty9cuLCuY/39738P1rZu3Rqs/fWvfw3WjjnmmGDt9ddfr60x9FTYrJ54YuWW8rS82MiRI4O1GTNmBGvV/p/cZZddGuopa4MGDQrWpkyZEqztv//+wdqECRMa6ikNvQ667r5MkgXKlRfTA5AJ8goUA1kF0sE7owEAACBKDLoAAACIEoMuAAAAosSgCwAAgCgx6AIAACBKtSwvBgCFs+OO6f54q7Z0z5o1a4K10aNHV9x+yy231NXH008/HaxdeOGFwdqyZcuCtQEDBtTVC+J0+umnV9z+/PPPB28ze/bsYO273/1usPbiiy8Ga/vtt1+wdvXVVwdr1ZYewyfts88+wdpdd92VYif14RldAAAARIlBFwAAAFFi0AUAAECUGHQBAAAQJQZdAAAARIlBFwAAAFFiebGMdXV1BWvVliSq5qCDDqq3HSAaRx99dLA2c+bMYO3GG28M1iZNmhSsLVmyJFhraWkJ1pJWrcfHHnssWFu+fHmwNmrUqIZ6QlwGDhxYcfvPfvaz4G0uv/zyYG3XXXcN1j744INgbaeddgrWkI7jjz8+6xZ6xTO6AAAAiBKDLgAAAKLEoAsAAIAoMegCAAAgSgy6AAAAiBKrLmRsy5YtwdpZZ51V1z5vuummetsB+oXrr7++rlrMJk6cmHULKLjQagySNGTIkLr2ycoKaBTP6AIAACBKDLoAAACIEoMuAAAAosSgCwAAgCgx6AIAACBKDLoAAACIUq/Li5nZaEnzJI2UtE3SHHe/0cwul/RvkjrLV73E3R9sVqOxGjRoULA2e/bsYG3GjBnNaAcFR16BYiCrQDpqWUd3q6Tz3H2Fme0i6Rkze6Rcu8Hdf9689gD0EXkFioGsAinoddB19w2SNpQvv2tmL0ga1ezGAPQdeQWKgawC6ejTObpmNkbSBElPlTdNN7PVZna7me2WcG8AGkBegWIgq0Dz1DzomtlgSQslnevumyX9UtJYSeNV+q30usDtzjSzdjNr7+zsrHQVAAkjr0AxkFWguWoadM1soEpBnO/uv5Mkd3/L3bvcfZukWyRNqnRbd5/j7m3u3tba2ppU3wACyCtQDGQVaL5eB10zM0m3SXrB3a/vtn2Pblf7lqTnkm8PQF+QV6AYyCqQjlpWXThY0qmSnjWzVeVtl0g62czGS3JJHZLOakqHkRswYECwdvbZZ9dVQ79GXoFiIKtACmpZdWGZJKtQYl0/IGfIK1AMZBVIB++MBgAAgCgx6AIAACBKDLoAAACIEoMuAAAAosSgCwAAgCgx6AIAACBKDLoAAACIEoMuAAAAosSgCwAAgCgx6AIAACBKDLoAAACIEoMuAAAAomTunt7BzDolrS1/OUzS26kdvLq89JKXPqT89BJbH59z99YE9tN0Oc1rXvqQ8tNLXvqQ8tNLEn0UNatSXP8OSclLL/TxSU3Na6qD7scObNbu7m2ZHLyHvPSSlz6k/PRCH/mQl/uflz6k/PSSlz6k/PSSlz6ykpf7n5c+pPz0Qh+f1OxeOHUBAAAAUWLQBQAAQJSyHHTnZHjsnvLSS176kPLTC33kQ17uf176kPLTS176kPLTS176yEpe7n9e+pDy0wt9fFJTe8nsHF0AAACgmTh1AQAAAFFi0AUAAECUMhl0zewoM3vRzF4xs4uy6KHcR4eZPWtmq8ysPeVj325mG83suW7bhprZI2b2cvnzbhn1cbmZvVF+XFaZ2dEp9DHazB4zsxfMbI2ZzSxvz+IxCfWS+uOSB+SVvFboIxd5Jasfl5eslnvJJK95yWqVXshrynlN/RxdMxsg6SVJR0haL2m5pJPd/flUGyn10iGpzd1TXzTZzA6V9DdJ89z9C+Vt10p6x91nlX9I7ebuF2bQx+WS/ubuP2/msXv0sYekPdx9hZntIukZSd+UdJrSf0xCvXxbKT8uWSOv/39s8vrxPnKRV7L6kTxltdxPhzLIa16yWqWXy0VeU81rFs/oTpL0iru/5u5bJN0l6fgM+siUuz8h6Z0em4+XdEf58h0qfQNk0Ufq3H2Du68oX35X0guSRimbxyTUS39EXkVeK/SRi7yS1Y8hq8pPVqv0krr+ntcsBt1RktZ1+3q9svvB5JIWm9kzZnZmRj10N8LdN0ilbwhJwzPsZbqZrS6/9JLKyzzbmdkYSRMkPaWMH5MevUgZPi4ZIa9h5FX5yStZzVVWpXzlNU9ZlchrqnnNYtC1CtuyWuPsYHf/kqSvSzqn/DIDpF9KGitpvKQNkq5L68BmNljSQknnuvvmtI5bYy+ZPS4ZIq/51+/zSlYl5SurEnkNIa8p5zWLQXe9pNHdvv6spDcz6EPu/mb580ZJ96r00k+W3iqfw7L9XJaNWTTh7m+5e5e7b5N0i1J6XMxsoErf/PPd/XflzZk8JpV6yepxyRh5DSOvOcgrWf1/ucmqlLu85iKrEnnNIq9ZDLrLJe1tZnuZ2SBJJ0m6P+0mzKylfDK0zKxF0pGSnqt+q6a7X9K08uVpku7Loont3/hl31IKj4uZmaTbJL3g7td3K6X+mIR6yeJxyQHyGkZeM84rWf2YXGRVymVec5FVibxW6qPpj4m7p/4h6WiV/jr0VUn/nlEP/yzpf8ofa9LuQ9JvVHqK/kOVfhP/vqTdJT0q6eXy56EZ9XGnpGclrVYpCHuk0MchKr3MtlrSqvLH0Rk9JqFeUn9c8vBBXslrhT5ykVey+onHI/OslvvILK95yWqVXshrynnlLYABAAAQJd4ZDQAAAFFi0AUAAECUGHQBAAAQJQZdAAAARIlBFwAAAFFi0AUAAECUGHQBAAAQpf8DSgsl1JBI11MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(157)\n",
    "\n",
    "idxs = np.random.randint(0, train_data.shape[0], 9)\n",
    "\n",
    "fig, ax = plt.subplots(3, 3, figsize=(10, 10))\n",
    "\n",
    "ax[0, 0].imshow(train_data[idxs[0], :, :], cmap='Greys')\n",
    "ax[0, 0].set_title('Handwritten number ' + str(train_labels[idxs[0]]))\n",
    "\n",
    "ax[0, 1].imshow(train_data[idxs[1], :, :], cmap='Greys')\n",
    "ax[0, 1].set_title('Handwritten number ' + str(train_labels[idxs[1]]))\n",
    "\n",
    "ax[0, 2].imshow(train_data[idxs[2], :, :], cmap='Greys')\n",
    "ax[0, 2].set_title('Handwritten number ' + str(train_labels[idxs[2]]))\n",
    "\n",
    "ax[1, 0].imshow(train_data[idxs[3], :, :], cmap='Greys')\n",
    "ax[1, 0].set_title('Handwritten number ' + str(train_labels[idxs[3]]))\n",
    "\n",
    "ax[1, 1].imshow(train_data[idxs[4], :, :], cmap='Greys')\n",
    "ax[1, 1].set_title('Handwritten number ' + str(train_labels[idxs[4]]))\n",
    "\n",
    "ax[1, 2].imshow(train_data[idxs[5], :, :], cmap='Greys')\n",
    "ax[1, 2].set_title('Handwritten number ' + str(train_labels[idxs[5]]))\n",
    "\n",
    "ax[2, 0].imshow(train_data[idxs[6], :, :], cmap='Greys')\n",
    "ax[2, 0].set_title('Handwritten number ' + str(train_labels[idxs[6]]))\n",
    "\n",
    "ax[2, 1].imshow(train_data[idxs[7], :, :], cmap='Greys')\n",
    "ax[2, 1].set_title('Handwritten number ' + str(train_labels[idxs[7]]))\n",
    "\n",
    "ax[2, 2].imshow(train_data[idxs[8], :, :], cmap='Greys')\n",
    "ax[2, 2].set_title('Handwritten number ' + str(train_labels[idxs[8]]))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027687,
     "end_time": "2021-01-14T21:33:02.168586",
     "exception": false,
     "start_time": "2021-01-14T21:33:02.140899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As we can see, some of these numbers are written with very poor handwritting that can even confuse a human, meaning the CNN will have quite a difficult task to acomplish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025762,
     "end_time": "2021-01-14T21:33:02.219678",
     "exception": false,
     "start_time": "2021-01-14T21:33:02.193916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025514,
     "end_time": "2021-01-14T21:33:02.271200",
     "exception": false,
     "start_time": "2021-01-14T21:33:02.245686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preparing the data for classification\n",
    "\n",
    "There are a couple of preprocessing steps we need to complete before the training of the model:\n",
    "\n",
    "* First, the training labels need to be one-hot encoded so we can use a softmax output layer at the end of the network.\n",
    "* Next, we need to normalize the values of the data between 0 and 1, which leads to an improvement of the results of the classification.\n",
    "* We also need to reshape the data again by adding a new fourth axis which represents the color channel of the image.\n",
    "* Finally, we need to create a new dataset which will be used for validation during the training phase of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026783,
     "end_time": "2021-01-14T21:33:02.323801",
     "exception": false,
     "start_time": "2021-01-14T21:33:02.297018",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### One-hot encoding the labels\n",
    "\n",
    "One-hot encodin the labels so we can use the *softmax* output layer with the *categorical crossentropy* loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T21:33:02.382043Z",
     "iopub.status.busy": "2021-01-14T21:33:02.381197Z",
     "iopub.status.idle": "2021-01-14T21:33:03.149837Z",
     "shell.execute_reply": "2021-01-14T21:33:03.150484Z"
    },
    "papermill": {
     "duration": 0.800821,
     "end_time": "2021-01-14T21:33:03.150635",
     "exception": false,
     "start_time": "2021-01-14T21:33:02.349814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "train_labels = encoder.fit_transform(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.060509,
     "end_time": "2021-01-14T21:33:03.238906",
     "exception": false,
     "start_time": "2021-01-14T21:33:03.178397",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Normalization\n",
    "\n",
    "Scaling the values of the pixels of the images in range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T21:33:03.298515Z",
     "iopub.status.busy": "2021-01-14T21:33:03.297358Z",
     "iopub.status.idle": "2021-01-14T21:33:03.425995Z",
     "shell.execute_reply": "2021-01-14T21:33:03.425336Z"
    },
    "papermill": {
     "duration": 0.16068,
     "end_time": "2021-01-14T21:33:03.426134",
     "exception": false,
     "start_time": "2021-01-14T21:33:03.265454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = train_data.astype('float32') / 255\n",
    "test_data = test_data.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025272,
     "end_time": "2021-01-14T21:33:03.477892",
     "exception": false,
     "start_time": "2021-01-14T21:33:03.452620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Reshaping the data\n",
    "\n",
    "2D convolutional neural networks require the images to be 3D, where one of the dimensions represents the colour channel. Seeing as our images are black and white, we simply add an empty new axis at the end of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T21:33:03.533990Z",
     "iopub.status.busy": "2021-01-14T21:33:03.533343Z",
     "iopub.status.idle": "2021-01-14T21:33:03.537909Z",
     "shell.execute_reply": "2021-01-14T21:33:03.537394Z"
    },
    "papermill": {
     "duration": 0.03462,
     "end_time": "2021-01-14T21:33:03.538011",
     "exception": false,
     "start_time": "2021-01-14T21:33:03.503391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = train_data[:, :, :, np.newaxis]\n",
    "test_data = test_data[:, :, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025969,
     "end_time": "2021-01-14T21:33:03.589893",
     "exception": false,
     "start_time": "2021-01-14T21:33:03.563924",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Creating the validation dataset\n",
    "\n",
    "During the training, the network requires a validation dataset to be able to calculate its performace. We can create this dataset by setting aside 10% of the training data to be used as validation, using the *train_test_split* function from the *sklearn* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T21:33:03.647731Z",
     "iopub.status.busy": "2021-01-14T21:33:03.646870Z",
     "iopub.status.idle": "2021-01-14T21:33:04.871749Z",
     "shell.execute_reply": "2021-01-14T21:33:04.870960Z"
    },
    "papermill": {
     "duration": 1.256188,
     "end_time": "2021-01-14T21:33:04.871872",
     "exception": false,
     "start_time": "2021-01-14T21:33:03.615684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.1, random_state=157, stratify=train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027988,
     "end_time": "2021-01-14T21:33:04.927690",
     "exception": false,
     "start_time": "2021-01-14T21:33:04.899702",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Before training, a good practice to use (especially when working with image data) is to perform data augmentation. This can be easily done using the *ImageDataGenerator* function from the *Keras* library.\n",
    "\n",
    "We augment the images in the training set by performing certain transformations upon them, such as:\n",
    "\n",
    "* Rotating by 10 degrees\n",
    "* Zooming by 10%\n",
    "* Horizontal and vertical shift of the image by 10%\n",
    "\n",
    "Flipping and whitening of the images is not performed due to the possibilty of hurting the final results rather than aiding them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T21:33:04.988775Z",
     "iopub.status.busy": "2021-01-14T21:33:04.987982Z",
     "iopub.status.idle": "2021-01-14T21:33:10.022283Z",
     "shell.execute_reply": "2021-01-14T21:33:10.021211Z"
    },
    "papermill": {
     "duration": 5.067509,
     "end_time": "2021-01-14T21:33:10.022403",
     "exception": false,
     "start_time": "2021-01-14T21:33:04.954894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "data_generator = ImageDataGenerator(\n",
    "        featurewise_center=False,\n",
    "        samplewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=False,\n",
    "        horizontal_flip=False,\n",
    "        vertical_flip=False,\n",
    "        rotation_range=10,\n",
    "        zoom_range = 0.1,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1\n",
    "        )\n",
    "\n",
    "\n",
    "data_generator.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02713,
     "end_time": "2021-01-14T21:33:10.076396",
     "exception": false,
     "start_time": "2021-01-14T21:33:10.049266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Building the Keras CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T21:33:10.153714Z",
     "iopub.status.busy": "2021-01-14T21:33:10.152930Z",
     "iopub.status.idle": "2021-01-14T21:33:13.236338Z",
     "shell.execute_reply": "2021-01-14T21:33:13.235594Z"
    },
    "papermill": {
     "duration": 3.13396,
     "end_time": "2021-01-14T21:33:13.236474",
     "exception": false,
     "start_time": "2021-01-14T21:33:10.102514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, Activation, Dropout, Dense, MaxPooling2D, Conv2D, Flatten\n",
    "from keras.initializers import he_uniform, glorot_uniform\n",
    "from keras.activations import relu\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "init_relu = he_uniform(seed=157)        # used for ReLU\n",
    "init_tanh = glorot_uniform(seed=157)    # used for tanh and softmax\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# input and first Convolutional layer\n",
    "model.add(Conv2D(name='Conv_1', input_shape=(28, 28, 1), filters=64, kernel_size=(3, 3), padding='same', kernel_initializer=init_relu, kernel_constraint=maxnorm(3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(relu))\n",
    "\n",
    "\n",
    "# second Convolutional layer and first MaxPooling layer\n",
    "model.add(Conv2D(name='Conv_2', filters=64, kernel_size=(3, 3), padding='same', kernel_initializer=init_relu, kernel_constraint=maxnorm(3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(relu))\n",
    "model.add(MaxPooling2D(name='Pooling_1', pool_size=(2, 2), strides=2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "\n",
    "# fourth Convolutional layer\n",
    "model.add(Conv2D(name='Conv_4', filters=32, kernel_size=(3, 3), padding='same', kernel_initializer=init_relu, kernel_constraint=maxnorm(3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(relu))\n",
    "\n",
    "\n",
    "# fifth Convolutional layer and second MaxPooling layer\n",
    "model.add(Conv2D(name='Conv_5', filters=32, kernel_size=(3, 3), padding='same', kernel_initializer=init_relu, kernel_constraint=maxnorm(3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(relu))\n",
    "model.add(MaxPooling2D(name='Pooling_2', pool_size=(2, 2), strides=2))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "\n",
    "# first Dense layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(name='FC_1', units=512, kernel_initializer=init_relu, kernel_constraint=maxnorm(3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(relu))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "# second Dense layer\n",
    "model.add(Dense(name='FC_2', units=128, kernel_initializer=init_relu, kernel_constraint=maxnorm(3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(relu))\n",
    "\n",
    "# softmax output Dense layer\n",
    "model.add(Dense(name='output', units=10, activation='softmax', kernel_initializer=init_tanh, kernel_constraint=maxnorm(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026663,
     "end_time": "2021-01-14T21:33:13.289651",
     "exception": false,
     "start_time": "2021-01-14T21:33:13.262988",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setting all random variables to a single seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T21:33:13.355466Z",
     "iopub.status.busy": "2021-01-14T21:33:13.353639Z",
     "iopub.status.idle": "2021-01-14T21:33:13.360888Z",
     "shell.execute_reply": "2021-01-14T21:33:13.361770Z"
    },
    "papermill": {
     "duration": 0.046099,
     "end_time": "2021-01-14T21:33:13.361893",
     "exception": false,
     "start_time": "2021-01-14T21:33:13.315794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ['PYTHONHASHSEED'] = str(157)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(157)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(157)\n",
    "\n",
    "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.compat.v1.set_random_seed(157)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "from tensorflow.python.keras import backend as k\n",
    "\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                                        inter_op_parallelism_threads=1)\n",
    "\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "k.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02871,
     "end_time": "2021-01-14T21:33:13.416760",
     "exception": false,
     "start_time": "2021-01-14T21:33:13.388050",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T21:33:13.486790Z",
     "iopub.status.busy": "2021-01-14T21:33:13.485931Z",
     "iopub.status.idle": "2021-01-14T21:45:52.736794Z",
     "shell.execute_reply": "2021-01-14T21:45:52.735172Z"
    },
    "papermill": {
     "duration": 759.29397,
     "end_time": "2021-01-14T21:45:52.736942",
     "exception": false,
     "start_time": "2021-01-14T21:33:13.442972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.2687 - accuracy: 0.9189\n",
      "Epoch 00001: val_loss improved from inf to 0.08681, saving model to neural_network_checkpoint_training.h5\n",
      "591/591 [==============================] - 16s 27ms/step - loss: 0.2684 - accuracy: 0.9190 - val_loss: 0.0868 - val_accuracy: 0.9729\n",
      "Epoch 2/50\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.0929 - accuracy: 0.9723\n",
      "Epoch 00002: val_loss improved from 0.08681 to 0.05786, saving model to neural_network_checkpoint_training.h5\n",
      "591/591 [==============================] - 16s 28ms/step - loss: 0.0931 - accuracy: 0.9722 - val_loss: 0.0579 - val_accuracy: 0.9817\n",
      "Epoch 3/50\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9789\n",
      "Epoch 00003: val_loss improved from 0.05786 to 0.03251, saving model to neural_network_checkpoint_training.h5\n",
      "591/591 [==============================] - 15s 26ms/step - loss: 0.0699 - accuracy: 0.9789 - val_loss: 0.0325 - val_accuracy: 0.9893\n",
      "Epoch 4/50\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9819\n",
      "Epoch 00004: val_loss improved from 0.03251 to 0.02748, saving model to neural_network_checkpoint_training.h5\n",
      "591/591 [==============================] - 15s 26ms/step - loss: 0.0597 - accuracy: 0.9819 - val_loss: 0.0275 - val_accuracy: 0.9910\n",
      "Epoch 5/50\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9848\n",
      "Epoch 00005: val_loss did not improve from 0.02748\n",
      "591/591 [==============================] - 15s 25ms/step - loss: 0.0492 - accuracy: 0.9848 - val_loss: 0.0306 - val_accuracy: 0.9900\n",
      "Epoch 6/50\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.0481 - accuracy: 0.9847\n",
      "Epoch 00006: val_loss improved from 0.02748 to 0.02609, saving model to neural_network_checkpoint_training.h5\n",
      "591/591 [==============================] - 16s 27ms/step - loss: 0.0481 - accuracy: 0.9847 - val_loss: 0.0261 - val_accuracy: 0.9921\n",
      "Epoch 7/50\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.0401 - accuracy: 0.9875\n",
      "Epoch 00007: val_loss did not improve from 0.02609\n",
      "591/591 [==============================] - 14s 24ms/step - loss: 0.0401 - accuracy: 0.9876 - val_loss: 0.0339 - val_accuracy: 0.9905\n",
      "Epoch 8/50\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.0387 - accuracy: 0.9872\n",
      "Epoch 00008: val_loss improved from 0.02609 to 0.01898, saving model to neural_network_checkpoint_training.h5\n",
      "591/591 [==============================] - 15s 25ms/step - loss: 0.0386 - accuracy: 0.9872 - val_loss: 0.0190 - val_accuracy: 0.9914\n",
      "Epoch 9/50\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.0333 - accuracy: 0.9902\n",
      "Epoch 00009: val_loss did not improve from 0.01898\n",
      "591/591 [==============================] - 16s 26ms/step - loss: 0.0333 - accuracy: 0.9902 - val_loss: 0.0201 - val_accuracy: 0.9931\n",
      "Epoch 10/50\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.0313 - accuracy: 0.9901\n",
      "Epoch 00010: val_loss improved from 0.01898 to 0.01662, saving model to neural_network_checkpoint_training.h5\n",
      "591/591 [==============================] - 15s 25ms/step - loss: 0.0313 - accuracy: 0.9901 - val_loss: 0.0166 - val_accuracy: 0.9950\n",
      "Epoch 11/50\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.0288 - accuracy: 0.9907\n",
      "Epoch 00011: val_loss improved from 0.01662 to 0.01588, saving model to neural_network_checkpoint_training.h5\n",
      "591/591 [==============================] - 15s 25ms/step - loss: 0.0289 - accuracy: 0.9907 - val_loss: 0.0159 - val_accuracy: 0.9948\n",
      "Epoch 12/50\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.0291 - accuracy: 0.9908\n",
      "Epoch 00012: val_loss did not improve from 0.01588\n",
      "591/591 [==============================] - 14s 24ms/step - loss: 0.0293 - accuracy: 0.9908 - val_loss: 0.0235 - val_accuracy: 0.9921\n",
      "Epoch 13/50\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.0295 - accuracy: 0.9914\n",
      "Epoch 00013: val_loss did not improve from 0.01588\n",
      "591/591 [==============================] - 15s 25ms/step - loss: 0.0295 - accuracy: 0.9914 - val_loss: 0.0173 - val_accuracy: 0.9936\n",
      "Epoch 14/50\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.0259 - accuracy: 0.9921\n",
      "Epoch 00014: val_loss did not improve from 0.01588\n",
      "591/591 [==============================] - 14s 24ms/step - loss: 0.0259 - accuracy: 0.9921 - val_loss: 0.0168 - val_accuracy: 0.9936\n",
      "Epoch 15/50\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9917\n",
      "Epoch 00015: val_loss did not improve from 0.01588\n",
      "591/591 [==============================] - 15s 26ms/step - loss: 0.0254 - accuracy: 0.9917 - val_loss: 0.0162 - val_accuracy: 0.9950\n",
      "Epoch 16/50\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.0235 - accuracy: 0.9922\n",
      "Epoch 00016: val_loss did not improve from 0.01588\n",
      "591/591 [==============================] - 13s 23ms/step - loss: 0.0235 - accuracy: 0.9922 - val_loss: 0.0173 - val_accuracy: 0.9952\n",
      "Epoch 17/50\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.0214 - accuracy: 0.9934\n",
      "Epoch 00017: val_loss did not improve from 0.01588\n",
      "591/591 [==============================] - 14s 24ms/step - loss: 0.0215 - accuracy: 0.9934 - val_loss: 0.0167 - val_accuracy: 0.9952\n",
      "Epoch 18/50\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9926\n",
      "Epoch 00018: val_loss did not improve from 0.01588\n",
      "591/591 [==============================] - 15s 26ms/step - loss: 0.0232 - accuracy: 0.9926 - val_loss: 0.0161 - val_accuracy: 0.9948\n",
      "Epoch 19/50\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9933\n",
      "Epoch 00019: val_loss improved from 0.01588 to 0.01549, saving model to neural_network_checkpoint_training.h5\n",
      "591/591 [==============================] - 14s 24ms/step - loss: 0.0200 - accuracy: 0.9933 - val_loss: 0.0155 - val_accuracy: 0.9960\n",
      "Epoch 20/50\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9935\n",
      "Epoch 00020: val_loss improved from 0.01549 to 0.01498, saving model to neural_network_checkpoint_training.h5\n",
      "591/591 [==============================] - 14s 24ms/step - loss: 0.0206 - accuracy: 0.9935 - val_loss: 0.0150 - val_accuracy: 0.9957\n",
      "Epoch 21/50\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9933\n",
      "Epoch 00021: val_loss did not improve from 0.01498\n",
      "591/591 [==============================] - 14s 24ms/step - loss: 0.0211 - accuracy: 0.9933 - val_loss: 0.0170 - val_accuracy: 0.9952\n",
      "Epoch 22/50\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.0195 - accuracy: 0.9943\n",
      "Epoch 00022: val_loss did not improve from 0.01498\n",
      "591/591 [==============================] - 15s 25ms/step - loss: 0.0195 - accuracy: 0.9943 - val_loss: 0.0197 - val_accuracy: 0.9943\n",
      "Epoch 23/50\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.0194 - accuracy: 0.9936\n",
      "Epoch 00023: val_loss did not improve from 0.01498\n",
      "591/591 [==============================] - 13s 22ms/step - loss: 0.0194 - accuracy: 0.9936 - val_loss: 0.0162 - val_accuracy: 0.9945\n",
      "Epoch 24/50\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.0176 - accuracy: 0.9947\n",
      "Epoch 00024: val_loss improved from 0.01498 to 0.01483, saving model to neural_network_checkpoint_training.h5\n",
      "591/591 [==============================] - 15s 26ms/step - loss: 0.0176 - accuracy: 0.9947 - val_loss: 0.0148 - val_accuracy: 0.9955\n",
      "Epoch 25/50\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.0170 - accuracy: 0.9944\n",
      "Epoch 00025: val_loss improved from 0.01483 to 0.01355, saving model to neural_network_checkpoint_training.h5\n",
      "591/591 [==============================] - 49s 83ms/step - loss: 0.0170 - accuracy: 0.9944 - val_loss: 0.0135 - val_accuracy: 0.9950\n",
      "Epoch 26/50\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.0164 - accuracy: 0.9947\n",
      "Epoch 00026: val_loss did not improve from 0.01355\n",
      "591/591 [==============================] - 15s 25ms/step - loss: 0.0164 - accuracy: 0.9947 - val_loss: 0.0158 - val_accuracy: 0.9945\n",
      "Epoch 27/50\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9950\n",
      "Epoch 00027: val_loss did not improve from 0.01355\n",
      "591/591 [==============================] - 13s 22ms/step - loss: 0.0164 - accuracy: 0.9950 - val_loss: 0.0166 - val_accuracy: 0.9948\n",
      "Epoch 28/50\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.0159 - accuracy: 0.9950\n",
      "Epoch 00028: val_loss improved from 0.01355 to 0.01301, saving model to neural_network_checkpoint_training.h5\n",
      "591/591 [==============================] - 14s 23ms/step - loss: 0.0159 - accuracy: 0.9950 - val_loss: 0.0130 - val_accuracy: 0.9957\n",
      "Epoch 29/50\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.0172 - accuracy: 0.9948\n",
      "Epoch 00029: val_loss did not improve from 0.01301\n",
      "591/591 [==============================] - 15s 26ms/step - loss: 0.0172 - accuracy: 0.9948 - val_loss: 0.0138 - val_accuracy: 0.9943\n",
      "Epoch 30/50\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.0156 - accuracy: 0.9950\n",
      "Epoch 00030: val_loss did not improve from 0.01301\n",
      "591/591 [==============================] - 13s 22ms/step - loss: 0.0156 - accuracy: 0.9949 - val_loss: 0.0144 - val_accuracy: 0.9945\n",
      "Epoch 31/50\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9952\n",
      "Epoch 00031: val_loss did not improve from 0.01301\n",
      "591/591 [==============================] - 13s 23ms/step - loss: 0.0146 - accuracy: 0.9952 - val_loss: 0.0139 - val_accuracy: 0.9945\n",
      "Epoch 32/50\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.0147 - accuracy: 0.9955\n",
      "Epoch 00032: val_loss did not improve from 0.01301\n",
      "591/591 [==============================] - 14s 23ms/step - loss: 0.0146 - accuracy: 0.9955 - val_loss: 0.0147 - val_accuracy: 0.9955\n",
      "Epoch 33/50\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.0153 - accuracy: 0.9949\n",
      "Epoch 00033: val_loss did not improve from 0.01301\n",
      "591/591 [==============================] - 13s 22ms/step - loss: 0.0152 - accuracy: 0.9949 - val_loss: 0.0140 - val_accuracy: 0.9950\n",
      "Epoch 34/50\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9954\n",
      "Epoch 00034: val_loss did not improve from 0.01301\n",
      "591/591 [==============================] - 16s 26ms/step - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.0137 - val_accuracy: 0.9948\n",
      "Epoch 35/50\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.0138 - accuracy: 0.9955\n",
      "Epoch 00035: val_loss did not improve from 0.01301\n",
      "591/591 [==============================] - 13s 22ms/step - loss: 0.0138 - accuracy: 0.9955 - val_loss: 0.0132 - val_accuracy: 0.9957\n",
      "Epoch 36/50\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9952\n",
      "Epoch 00036: val_loss did not improve from 0.01301\n",
      "591/591 [==============================] - 14s 24ms/step - loss: 0.0147 - accuracy: 0.9952 - val_loss: 0.0144 - val_accuracy: 0.9952\n",
      "Epoch 37/50\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9958\n",
      "Epoch 00037: val_loss did not improve from 0.01301\n",
      "591/591 [==============================] - 13s 23ms/step - loss: 0.0134 - accuracy: 0.9958 - val_loss: 0.0147 - val_accuracy: 0.9955\n",
      "Epoch 38/50\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9959\n",
      "Epoch 00038: val_loss did not improve from 0.01301\n",
      "591/591 [==============================] - 15s 26ms/step - loss: 0.0130 - accuracy: 0.9959 - val_loss: 0.0142 - val_accuracy: 0.9952\n",
      "Epoch 39/50\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.0134 - accuracy: 0.9956\n",
      "Epoch 00039: val_loss did not improve from 0.01301\n",
      "591/591 [==============================] - 13s 21ms/step - loss: 0.0134 - accuracy: 0.9956 - val_loss: 0.0137 - val_accuracy: 0.9957\n",
      "Epoch 40/50\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9958\n",
      "Epoch 00040: val_loss did not improve from 0.01301\n",
      "591/591 [==============================] - 13s 22ms/step - loss: 0.0130 - accuracy: 0.9958 - val_loss: 0.0135 - val_accuracy: 0.9957\n",
      "Epoch 41/50\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9959\n",
      "Epoch 00041: val_loss did not improve from 0.01301\n",
      "591/591 [==============================] - 15s 25ms/step - loss: 0.0125 - accuracy: 0.9959 - val_loss: 0.0131 - val_accuracy: 0.9962\n",
      "Epoch 42/50\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.0126 - accuracy: 0.9959\n",
      "Epoch 00042: val_loss improved from 0.01301 to 0.01252, saving model to neural_network_checkpoint_training.h5\n",
      "591/591 [==============================] - 13s 22ms/step - loss: 0.0126 - accuracy: 0.9959 - val_loss: 0.0125 - val_accuracy: 0.9957\n",
      "Epoch 43/50\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.0121 - accuracy: 0.9960\n",
      "Epoch 00043: val_loss improved from 0.01252 to 0.01217, saving model to neural_network_checkpoint_training.h5\n",
      "591/591 [==============================] - 16s 26ms/step - loss: 0.0122 - accuracy: 0.9960 - val_loss: 0.0122 - val_accuracy: 0.9962\n",
      "Epoch 44/50\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9958\n",
      "Epoch 00044: val_loss improved from 0.01217 to 0.01148, saving model to neural_network_checkpoint_training.h5\n",
      "591/591 [==============================] - 13s 22ms/step - loss: 0.0134 - accuracy: 0.9958 - val_loss: 0.0115 - val_accuracy: 0.9964\n",
      "Epoch 45/50\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.0120 - accuracy: 0.9963\n",
      "Epoch 00045: val_loss did not improve from 0.01148\n",
      "591/591 [==============================] - 14s 25ms/step - loss: 0.0119 - accuracy: 0.9963 - val_loss: 0.0125 - val_accuracy: 0.9962\n",
      "Epoch 46/50\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.0114 - accuracy: 0.9963\n",
      "Epoch 00046: val_loss did not improve from 0.01148\n",
      "591/591 [==============================] - 13s 22ms/step - loss: 0.0114 - accuracy: 0.9963 - val_loss: 0.0133 - val_accuracy: 0.9957\n",
      "Epoch 47/50\n",
      "591/591 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9962\n",
      "Epoch 00047: val_loss did not improve from 0.01148\n",
      "591/591 [==============================] - 13s 21ms/step - loss: 0.0120 - accuracy: 0.9962 - val_loss: 0.0143 - val_accuracy: 0.9955\n",
      "Epoch 48/50\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.0119 - accuracy: 0.9960\n",
      "Epoch 00048: val_loss did not improve from 0.01148\n",
      "591/591 [==============================] - 16s 26ms/step - loss: 0.0119 - accuracy: 0.9961 - val_loss: 0.0125 - val_accuracy: 0.9957\n",
      "Epoch 49/50\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.0127 - accuracy: 0.9963\n",
      "Epoch 00049: val_loss did not improve from 0.01148\n",
      "591/591 [==============================] - 14s 23ms/step - loss: 0.0127 - accuracy: 0.9963 - val_loss: 0.0132 - val_accuracy: 0.9957\n",
      "Epoch 50/50\n",
      "590/591 [============================>.] - ETA: 0s - loss: 0.0117 - accuracy: 0.9961\n",
      "Epoch 00050: val_loss did not improve from 0.01148\n",
      "591/591 [==============================] - 14s 23ms/step - loss: 0.0117 - accuracy: 0.9961 - val_loss: 0.0127 - val_accuracy: 0.9955\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, LearningRateScheduler\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import save_model\n",
    "\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "opt = Adam(learning_rate=lr, amsgrad=True)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('neural_network_checkpoint_training.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "lr_decay = LearningRateScheduler(lambda x: lr * 0.95 ** x)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./logs',\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "\n",
    "model.fit_generator(data_generator.flow(X_train, y_train, batch_size=64),\n",
    "        epochs=50,\n",
    "        validation_data=(X_val, y_val),\n",
    "        shuffle=True,\n",
    "        callbacks=[tensorboard, checkpoint, lr_decay],\n",
    "        verbose=1)\n",
    "\n",
    "save_model(checkpoint.model, 'neural_network_latest_saved.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 4.056293,
     "end_time": "2021-01-14T21:46:00.641205",
     "exception": false,
     "start_time": "2021-01-14T21:45:56.584912",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The final prediction and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-14T21:46:08.278510Z",
     "iopub.status.busy": "2021-01-14T21:46:08.277553Z",
     "iopub.status.idle": "2021-01-14T21:46:10.728408Z",
     "shell.execute_reply": "2021-01-14T21:46:10.727309Z"
    },
    "papermill": {
     "duration": 6.253138,
     "end_time": "2021-01-14T21:46:10.728526",
     "exception": false,
     "start_time": "2021-01-14T21:46:04.475388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "clf = load_model('neural_network_checkpoint_training.h5')\n",
    "\n",
    "prediction = clf.predict(test_data)\n",
    "\n",
    "prediction = pd.DataFrame(np.argmax(prediction, axis=-1), columns=['Label'])\n",
    "img_idx = pd.DataFrame(np.arange(1, len(prediction) + 1), columns=['ImageId'])\n",
    "\n",
    "prediction = pd.concat([img_idx, prediction], axis=1)\n",
    "\n",
    "prediction.to_csv('submission_cnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 4.212962,
     "end_time": "2021-01-14T21:46:18.782586",
     "exception": false,
     "start_time": "2021-01-14T21:46:14.569624",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "With sufficient data augmentation and tuning of the architecture and hyperparameters of the Convolutional Neural Network, we achieve a significant accuracy of ~99.6%, which more than acceptable for the task at hand. This further proves the point that artificial intelligence is the best new approach to the problem of computer vision, that will only continue to get better and better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 815.951356,
   "end_time": "2021-01-14T21:46:23.943610",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-14T21:32:47.992254",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
